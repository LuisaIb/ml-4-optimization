---
title: "Machine Learning Project"
author: "Hammerer, Ibele, Janez, Romer, Steinwender"
date: "2023-07-29"
output:
  ioslides_presentation: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Forschungsfragen:
1. Ist der Einsatz eines Variational Autoencoder als Datenerhebungstrategie sinnvoll? (VAE Experiment)
2. Ist ein RNN besser als ein Gauss bei der Optimierung? (Bewertungsmatrix Optimierung)
3. Ist der Einsatz von RNN geeignet? (Versuchkriterien)





# Verworfene Ansätze
- VAE für Regression
- VAE wegen Dimensionsreduktion nicht für Datenerhebung geeignet
- Loss Function selber schreiben
- L-BFGS-B als Optimierer




# Investigation

## Phase 1 Datenerhebungsstrategien

Unterschiedliche Herangehensweisen zur Erstellung von Datenpunkten aus der Ground Truth Funktion:


### Random Sampling
Vorteile:
- Anpassungsfähigkeit: Die Funktion erlaubt es, die Anzahl der zu generierenden Datenpunkte n als Eingabeparameter anzugeben. Dadurch kann die Menge der generierten Daten leicht kontrolliert werden.
- Flexibilität bei der Funktionswahl: Die Funktion akzeptiert eine beliebige Funktion f, die abhängig von den Koordinaten x und y den Wert z berechnet. Dadurch kann die Funktion generateDataPoints vielseitig für verschiedene Anwendungen eingesetzt werden, indem man unterschiedliche Funktionen f übergeben kann.
- Zufällige Verteilung: Die Funktion runif() wird verwendet, um Zufallswerte für x und y zu generieren. Dadurch werden die Datenpunkte in einem zufälligen Bereich innerhalb der durch getLowerBoxConstraints(ftest) und getUpperBoxConstraints(ftest) definierten Box erzeugt. Dies kann hilfreich sein, um eine breitere Palette von Szenarien und Randbedingungen abzudecken.
- Einfachheit: Die Funktion verwendet grundlegende R-Funktionen, wie runif() und apply(), um die Datenpunkte zu generieren und den Funktionswert z zu berechnen. Dies macht die Funktion einfach und leicht verständlich.
- Geschwindigkeit: Die Funktion nutzt eingebaute R-Funktionen zur Generierung der Datenpunkte. Dadurch kann sie in der Regel effizient und schnell arbeiten, auch für größere Datenmengen.


Nachteile: 
- Begrenzte Datenerzeugung: Die Funktion generiert Datenpunkte, indem sie Zufallszahlen innerhalb einer definierten Box verwendet. Dies kann dazu führen, dass die Verteilung der generierten Daten nicht genau den tatsächlichen Daten entspricht. Die Daten könnten ungleichmäßig oder in bestimmten Bereichen dichter sein, was möglicherweise nicht die tatsächliche Verteilung der Daten in der realen Welt widerspiegelt. 
- Fixe Boxgrenzen: Die Funktion verwendet feste Grenzen (lower und upper) für die generierten Datenpunkte. Wenn die tatsächlichen Daten in einem anderen Bereich liegen oder sich die Verteilung im Laufe der Zeit ändert, könnten die generierten Datenpunkte möglicherweise nicht repräsentativ für das reale Szenario sein. 
- Zufälligkeit: Die Verwendung von Zufallszahlen zur Generierung der Datenpunkte kann dazu führen, dass bei jeder Ausführung der Funktion unterschiedliche Datenpunkte erzeugt werden. Dies kann die Reproduzierbarkeit von Experimenten oder Analysen erschweren. - Begrenzte Dimensionen: Die Funktion ist auf 2D-Datenpunkte beschränkt (x und y). Wenn du mehrdimensionale Datenpunkte generieren möchtest, müsstest du die Funktion entsprechend anpassen.


### Grid Sampling
Theorie:

Vorteile:
- Exploration des Parameterraums: Grid Sampling ermöglicht eine umfassende Erkundung des Parameterraums, indem es eine vordefinierte Menge von Parameterkombinationen erstellt. Dadurch werden verschiedene Kombinationen von Hyperparametern ausprobiert, um diejenigen zu finden, die das Modell am besten optimieren.
- Einfachheit und Einfachheit der Implementierung: Grid Sampling ist einfach zu implementieren und erfordert keine komplexen Optimierungsalgorithmen oder heuristische Ansätze. Es ist leicht verständlich und kann auch auf einfache Weise parallelisiert werden, um das Training zu beschleunigen.
- Reproduzierbarkeit: Grid Sampling ist deterministisch und reproduzierbar, da es jeden Punkt im Parameterraum genau einmal besucht. Dadurch können die Ergebnisse einfach wiederholt werden.
- Vergleichbarkeit: Durch die systematische Auswertung verschiedener Parameterkombinationen ermöglicht Grid Sampling einen direkten Vergleich der Leistung verschiedener Modelle unter den gleichen Bedingungen.
- Geringe Anforderungen an die Datenmenge: Grid Sampling kann auch mit begrenzten Datenmengen durchgeführt werden, da es nicht auf große Datenmengen angewiesen ist, um eine aussagekräftige Optimierung durchzuführen.

Nachteile:
- Hoher Rechenaufwand: Grid Sampling kann sehr zeitaufwändig sein, insbesondere bei großen Parameterräumen oder komplexen Modellen. Da es alle möglichen Kombinationen ausprobiert, steigt die Anzahl der Modelltrainings exponentiell mit der Anzahl der Hyperparameter und ihrer möglichen Werte.
- Ineffizient bei großen Parameterräumen: Wenn der Parameterraum groß ist oder wenn die Hyperparameter viele mögliche Werte haben, kann Grid Sampling sehr ineffizient sein. Die meisten der Parameterkombinationen werden möglicherweise nicht zu einer wesentlichen Verbesserung des Modells führen, dennoch werden sie alle evaluiert.
- Fehlende Berücksichtigung der Beziehungen zwischen Hyperparametern: Grid Sampling betrachtet jede Kombination von Hyperparametern unabhängig voneinander. Es berücksichtigt möglicherweise nicht, wie sich bestimmte Hyperparameterkombinationen gegenseitig beeinflussen oder welche Abhängigkeiten zwischen ihnen bestehen.
- Overfitting des Hyperparameterraums: Wenn Grid Sampling auf derselben Datensatzpartition ausgeführt wird, die auch für die Modelloptimierung verwendet wird, besteht die Gefahr von Overfitting des Hyperparameterraums. Es kann sein, dass Grid Search zufällig die besten Hyperparameterkombinationen für diese spezielle Datensatzpartition findet, die jedoch möglicherweise nicht gut auf anderen Daten verallgemeinern.
- Nicht geeignet für kontinuierliche Hyperparameter: Grid Sampling eignet sich am besten für diskrete Hyperparameter, bei denen die möglichen Werte aus einer festen Menge ausgewählt werden können. Für kontinuierliche Hyperparameter ist es nicht praktikabel, alle möglichen Werte im Parameterraum auszuprobieren.
- Fehlende Optimierung: Grid Search ist ein einfacher, nicht-optimierender Ansatz. Es garantiert nicht, dass die gefundenen Hyperparameter die besten für das Modell sind. Andere Optimierungsalgorithmen wie Random Search oder Bayesian Optimization können möglicherweise effizienter sein, um eine gute Kombination von Hyperparametern zu finden.


### Latin Hypercube Sampling
Theorie:
Das Latin Hypercube Sampling (LHS) ist ein statistisches Stichprobenverfahren, das in verschiedenen Bereichen wie der Umweltmodellierung, der pharmazeutischen Forschung und der Strukturanalyse eingesetzt wird[1][2][3][4]. Es handelt sich um eine Methode zur Erzeugung einer repräsentativen Stichprobe von Parameterwerten aus einer mehrdimensionalen Verteilung.

Bei der LHS wird der Parameterraum in gleich wahrscheinliche Intervalle oder Bins entlang jeder Dimension unterteilt. Aus jedem Bereich wird dann ein einzelner Zufallswert ausgewählt, um einen Stichprobenpunkt zu bilden. Der Grundgedanke von LHS besteht darin, sicherzustellen, dass die Stichprobenpunkte gleichmäßig über den Parameterraum verteilt sind, was den Stichprobenfehler verringert und eine repräsentativere Stichprobe ergibt.

Die Latin Hypercube Sampling Technik hat mehrere Vorteile gegenüber anderen Stichprobenverfahren:
- Sie ermöglicht eine effizientere Nutzung des Stichprobenumfangs im Vergleich zu einfachen Zufallsstichproben.
- Sie stellt sicher, dass der gesamte Parameterraum erforscht wird, wodurch das Risiko, wichtige Regionen zu übersehen, verringert wird.
- Sie reduziert die Korrelation zwischen den Eingangsvariablen, was für die Sensitivitätsanalyse und die Quantifizierung der Unsicherheit von Vorteil sein kann.

LHS ist in verschiedenen Anwendungen weit verbreitet, z. B. in der sensitivity analysis, uncertainty analysis, optimization und model calibration. Es hilft Forschern und Analytikern, das Verhalten komplexer Systeme zu untersuchen, indem es vielfältige und repräsentative Stichproben aus dem Parameterraum erzeugt.

Insgesamt ist das Latin Hypercube Sampling eine wertvolle Technik zur Erzeugung repräsentativer Stichproben aus mehrdimensionalen Verteilungen, die eine genauere und effizientere Analyse in verschiedenen Bereichen ermöglicht.

Citations:
[1] https://www.semanticscholar.org/paper/8e01e454497fcaed2962f73422498791c855f34c
[2] https://www.semanticscholar.org/paper/e9a5a3e8e750b3716046f24ef7bebbccad784721
[3] https://www.semanticscholar.org/paper/b90f8a36ac1c467d48326788716b14ff2c2caa0b
[4] https://www.semanticscholar.org/paper/b4e1b2c725aeeaa70a9ee31d752bb12c02a6086a



Vorteile: 

1. Effizientere Nutzung des Stichprobenumfangs: LHS stellt sicher, dass die Stichprobenpunkte gleichmäßig über den Parameterraum verteilt sind, was eine effizientere Nutzung des verfügbaren Stichprobenumfangs ermöglicht. Dies kann im Vergleich zu anderen Stichprobenverfahren zu genaueren und zuverlässigeren Ergebnissen mit einer geringeren Anzahl von Stichproben führen[1].
2. Erkundung des gesamten Parameterraums: LHS unterteilt den Parameterraum in gleich wahrscheinliche Intervalle oder Bins entlang jeder Dimension. Durch die Auswahl eines einzelnen Zufallswertes aus jedem Bereich stellt LHS sicher, dass der gesamte Parameterraum erforscht wird, wodurch das Risiko, wichtige Regionen zu übersehen, verringert wird. Dies ist besonders bei der Sensitivitätsanalyse und der Quantifizierung der Unsicherheit von Vorteil[1][2].
3. Geringere Korrelation zwischen Eingabevariablen: LHS zielt darauf ab, die Korrelation zwischen den Eingabevariablen zu verringern, was bei verschiedenen Anwendungen von Vorteil sein kann. Durch die Verringerung der Korrelation ermöglicht LHS eine umfassendere Erkundung des Parameterraums und liefert eine genauere Darstellung des Systemverhaltens[1][2].
4. Verringerung der Varianz: LHS ist ein Verfahren zur Verringerung der Varianz, das zur Verbesserung der Effizienz und Genauigkeit von Simulationen beitragen kann. Indem sie eine gleichmäßige Verteilung der Stichprobenpunkte sicherstellt, reduziert LHS den Stichprobenfehler und liefert eine repräsentativere Stichprobe des Parameterraums[2].
5. Anwendbarkeit auf höhere Dimensionen: LHS ist besonders nützlich bei höherdimensionalen Problemen, bei denen einfache Zufallsstichproben möglicherweise nicht effektiv sind. LHS erweitert das Konzept der Schichtung auf höhere Dimensionen und ermöglicht so eine effektivere Erkundung des Parameterraums[2].
Insgesamt bietet das Latin Hypercube Sampling mehrere Vorteile gegenüber anderen Stichprobenverfahren, darunter eine effizientere Nutzung des Stichprobenumfangs, die Erkundung des gesamten Parameterraums, eine geringere Korrelation zwischen den Eingangsvariablen und eine Verringerung der Varianz. Diese Vorteile machen LHS zu einem wertvollen Werkzeug in verschiedenen Bereichen, darunter Umweltmodellierung, pharmazeutische Forschung und Optimierung[1][2][3][4].

Citations:
[1] https://www.semanticscholar.org/paper/8e01e454497fcaed2962f73422498791c855f34c
[2] https://www.semanticscholar.org/paper/115fe63a273b9504d1f66f6a5c2c655382bafecf
[3] https://www.semanticscholar.org/paper/009d598d18d253a4b5e34b18dc103072b892c1d5
[4] https://www.semanticscholar.org/paper/0569f1ddce2c9ffb4870c2decdb47167bec6fcaa
[5] https://www.semanticscholar.org/paper/e9a5a3e8e750b3716046f24ef7bebbccad784721
[6] https://www.semanticscholar.org/paper/01773881b1d9811d0b9ff270c4947a55f1b92d5f

Nachteile: 

1. Komplexität in höheren Dimensionen: LHS ist zwar bei höherdimensionalen Problemen wirksam, kann aber mit zunehmender Anzahl der Dimensionen rechnerisch komplex werden. Die Aufteilung des Parameterraums in gleich wahrscheinliche Intervalle oder Bins wird schwieriger, und die Rechenkosten für die Erstellung einer repräsentativen Stichprobe können steigen[2].
2. Fehlen einer allgemeinen Theorie für die Schätzung von Konfidenzintervallen: In den Suchergebnissen wird erwähnt, dass es keine verifizierte allgemeine Theorie für die genaue Schätzung von Konfidenzintervallen (CI) in LHS gibt. Zwar haben jüngste numerische Experimente gezeigt, dass LHS für bestimmte Wahrscheinlichkeitsbereiche effizienter ist als einfache Zufallsstichproben (SRS), doch ist das Verhalten von CI-Schätzern in LHS noch immer ein Forschungsgebiet[3].
3. Einschränkungen der Effizienz bei extremen Wahrscheinlichkeiten: Der Effizienzvorteil von LHS gegenüber SRS nimmt ab, wenn man sich den Wahrscheinlichkeitsextremen 0 und 1 nähert. In diesen Extremfällen bietet LHS möglicherweise keine signifikanten Effizienzgewinne[3].
Es ist wichtig, darauf hinzuweisen, dass es sich bei diesen potenziellen Nachteilen nicht um inhärente Beschränkungen von LHS handelt, sondern vielmehr um Überlegungen, die in bestimmten Szenarien auftreten können. LHS ist nach wie vor ein wertvolles Stichprobenverfahren mit zahlreichen Vorteilen.


Citations:
[1] https://www.semanticscholar.org/paper/8e01e454497fcaed2962f73422498791c855f34c
[2] https://www.semanticscholar.org/paper/115fe63a273b9504d1f66f6a5c2c655382bafecf
[3] https://www.semanticscholar.org/paper/351f3f69e713ac4c7f1086b2ad9c08efdc87d868
[4] https://www.semanticscholar.org/paper/009d598d18d253a4b5e34b18dc103072b892c1d5
[5] https://arxiv.org/abs/2305.12043
[6] https://www.semanticscholar.org/paper/6d8a0e7b65bf147774991ed45560dbaee6361bf2


### Dimensionen

Funktion f1: Die Funktion f1 ist eine Benchmark-Funktion (Testfunktion) mit 2-dimensionalen Eingabedaten. Sie gehört zu den "BBOB" (Black-Box Optimization Benchmarking) Funktionen, die in der "smoof" Bibliothek enthalten sind. BBOB-Funktionen sind standardisierte Testfunktionen, die in der Optimierungsforschung verwendet werden, um Algorithmen für die globale Optimierung zu vergleichen und zu evaluieren. Die genaue mathematische Definition der f1-Funktion kann im Quellcode der "smoof" Bibliothek nachgesehen werden.

Funktion f4: Die Funktion f4 ist ebenfalls eine Benchmark-Funktion mit 2-dimensionalen Eingabedaten aus der BBOB-Funktionsgruppe. Wie f1 gehört sie zu den standardisierten Testfunktionen, die in der Optimierungsforschung verwendet werden. Genauere Details zur mathematischen Definition der f4-Funktion können im Quellcode der "smoof" Bibliothek gefunden werden.


------------------------------------------------------------------------------------------------------------------



## Phase 2 Modellentwicklung und Modellauswahl

### Variational Autoencoder
Theorie:
Ein Variational Autoencoder (VAE) ist eine Art künstliches neuronales Netz, das für die generative Modellierung von Daten verwendet wird[2][4]. Es besteht aus einem Encoder, einem Decoder und einer Verlustfunktion[2]. Der Encoder komprimiert Daten in einen latenten Raum, während der Decoder neue Daten aus dem latenten Raum erzeugt[2][5]. Die wichtigste Neuerung der VAEs ist die Hinzufügung der Variationsinferenz, die auf die Wahrscheinlichkeitsverteilung von Änderungen im Eingangsdatensignal einwirken kann[3]. Dadurch können VAEs realistisch aussehende Bilder und andere Arten von Daten erzeugen[3]. VAEs werden mittels Gradientenabstieg trainiert, um den Verlust in Bezug auf die Parameter des Encoders und Decoders zu optimieren[2]. Variationale Autoencoder werden oft mit dem Autoencoder-Modell in Verbindung gebracht, aber sie haben signifikante Unterschiede in Bezug auf das Ziel und die mathematische Formulierung[4].

Quellen:
[1] https://wr.informatik.uni-hamburg.de/_media/teaching/sommersemester_2021/siw-21-variational_autoencoder-cindy.pdf
[2] https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
[3] https://www.techtarget.com/searchenterpriseai/definition/variational-autoencoder-VAE
[4] https://en.wikipedia.org/wiki/Variational_autoencoder
[5] https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
[6] https://www.mdpi.com/1099-4300/24/3/423

Modelldefinition:
- Der Variational Autoencoder (VAE) besteht aus zwei Hauptkomponenten: einem Encoder-Netzwerk und einem Decoder-Netzwerk.
- Das Encoder-Netzwerk verwendet mehrere Dense-Layer, um die Eingabedaten schrittweise in eine niedrigdimensionale Latent-Space-Repräsentation zu transformieren.
- Das Decoder-Netzwerk ist das Gegenstück zum Encoder und rekonstruiert die ursprünglichen Eingabedaten aus der Latent-Space-Repräsentation.
- Der Latent-Space hat eine vorgegebene Dimension (latent_dim), die im Code auf 2 festgelegt ist.
- Die Latent-Space-Repräsentation wird durch den Mean (z_mean) und den Logarithmus der Varianz (z_log_var) definiert.

Encoder:
- Der erste Dense-Layer (h) hat intermediate_dim Neuronen und verwendet die ReLU-Aktivierungsfunktion, um nicht-lineare Eigenschaften der Daten zu erfassen.
- Der zweite Dense-Layer (i) hat ebenfalls intermediate_dim Neuronen und führt weitere nicht-lineare Transformationen mithilfe der ReLU-Aktivierungsfunktion durch.
- Die letzten beiden Dense-Layer (z_mean und z_log_var) bestehen aus latent_dim Neuronen, wobei latent_dim die Dimension des Latent-Space ist. z_mean repräsentiert den Mittelwert der Latent-Space-Repräsentation, während z_log_var den Logarithmus der Varianz darstellt.

Decoder:
- Der erste Dense-Layer (decoder_h) besteht aus intermediate_dim Neuronen und verwendet die ReLU-Aktivierungsfunktion für nicht-lineare Transformationen.
- Der zweite Dense-Layer (decoder_mean) hat input_dim Neuronen, wobei input_dim die Dimension der ursprünglichen Eingabedaten ist. Hier wird die lineare Aktivierungsfunktion verwendet, um die ursprünglichen Eingabedaten zu rekonstruieren.

Sampling-Funktion:
- Die Sampling-Funktion generiert Latent-Space-Punkte und verwendet z_mean und z_log_var als Eingabe.
- Die "Reparameterization Trick" wird angewendet, indem ein normalverteilter Zufallsvektor mit geringer Varianz (durch epsilon_std gesteuert) multipliziert und zum z_mean addiert wird.

Kompilierung des Modells:
- Das gesamte VAE-Modell wird durch Verketten von Encoder- und Decoder-Netzwerken erstellt.
- Das Modell wird mit dem VAE-Loss (vae_loss) kompiliert, der aus einer Kombination von Mean Squared Error (MSE) und Kullback-Leibler (KL)-Divergenz besteht. MSE misst den Rekonstruktionsfehler, während KL-Divergenz die Ähnlichkeit der Latent-Space-Repräsentation mit einer Standardnormalverteilung misst.

Training des Modells:
- Das VAE-Modell wird mit den Trainingsdaten (input_data) trainiert, die in der Variable dfF1 im Code enthalten sind.
- Der Adam-Optimizer mit einer Lernrate von 0.0001 wird verwendet.
- Das Training erfolgt über eine bestimmte Anzahl von Epochen (epochs) und in Batches (batch_size).
- Das Modell wird auch mit den Testdaten (test_data) validiert, um die Leistung zu überwachen.

------------------------------------------------------------------------


### Versuchsaufbau mit automatisierter Datenerhebung und RNN Modell
Theorie:
Ein rekurrentes neuronales Netz (RNN) ist eine Art künstliches neuronales Netz, das für die Verarbeitung sequenzieller Daten wie Zeitreihen oder natürliche Sprache konzipiert ist. Im Gegensatz zu neuronalen Feedforward-Netzen, die Daten in einem einzigen Durchgang von der Eingabe zur Ausgabe verarbeiten, verfügen RNNs über eine Rückkopplungsschleife, die es ermöglicht, dass Informationen bestehen bleiben und von einem Schritt zum nächsten weitergegeben werden[1].
Das Hauptmerkmal von RNNs ist ihre Fähigkeit, einen verborgenen Zustand beizubehalten, der Informationen aus früheren Schritten in der Sequenz aufnimmt. Dieser verborgene Zustand dient als eine Art Gedächtnis, das es dem Netz ermöglicht, bei der Verarbeitung der aktuellen Eingaben auf frühere Informationen zurückzugreifen. Dadurch eignen sich RNNs sehr gut für Aufgaben, die sequenzielle Abhängigkeiten beinhalten, wie z. B. Sprachmodellierung, Spracherkennung, maschinelle Übersetzung und Stimmungsanalyse[1].
Eine gängige Variante von RNNs ist das Long Short-Term Memory (LSTM)-Netz, das das Problem des verschwindenden Gradienten löst, das bei herkömmlichen RNNs auftreten kann. LSTM-Netzwerke führen zusätzliche Gating-Mechanismen ein, die den Informationsfluss durch das Netzwerk steuern und es ihm ermöglichen, sich selektiv an Informationen über lange Sequenzen zu erinnern oder diese zu vergessen[2].
Insgesamt sind RNNs und LSTM-Netze leistungsstarke Werkzeuge für die Modellierung sequenzieller Daten und werden in verschiedenen Bereichen häufig eingesetzt. Sie bieten eine Möglichkeit, zeitliche Abhängigkeiten zu erfassen, und haben den Bereich des Deep Learning entscheidend vorangebracht.

Citations:
[1] https://arxiv.org/abs/1810.10708
[2] https://arxiv.org/abs/2006.03860
[3] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10280473/
[4] https://www.semanticscholar.org/paper/366cc0e5e7db200e518184690a78d54a57c53306
[5] https://www.semanticscholar.org/paper/32e0b010950e31e77502c984c69a73ddf0dc4814



Erforderliche Pakete: Der Code beginnt mit dem Laden verschiedener R-Pakete, die für den Versuchsaufbau benötigt werden, darunter smoof für Benchmark-Testfunktionen, ggplot2 für Diagramme, keras für neuronale Netzwerke, SPOT für Optimierungsalgorithmen, pracma für praktische Mathematikfunktionen, lhs für Latin-Hypercube-Abtastung, COBBS für Modellierung von Funktionen und kernlab für Support Vector Machines und Gauß'sche Prozesse.

Einstellungen und Anpassungen: Hier werden verschiedene Parameter eingestellt, darunter die Anzahl der Benchmark-Testfunktionen (numBbobf), die Dimension der Funktionen (dim) - 2 und 3, die Methode zur Generierung von Daten (dataGenerationMethod) - Random Sampling, Grid Sampling und LHS, die Anzahl der Datenpunkte (numDataPoints) - 25 und 600, die Aufteilung von Trainings- und Testdaten (trainTestSplit) und die Anzahl der Funktionsauswertungen (funEval).

Laden der Funktionen: Die Funktionen für die Erstellung von Testfunktionen, die Erstellung von 2D-Funktionsplots und das Anzeigen der Testfunktionen werden definiert - f1 und f24.

Laden und Darstellen der Testfunktionen: Die ausgewählten Benchmark-Testfunktionen werden geladen und ihre unteren und oberen Schranken werden abgerufen. Wenn die Dimension 2 ist, wird eine Funktion zum Erstellen eines 2D-Plots der Testfunktionen aufgerufen.

Generieren der Trainingsdaten: Funktionen zum Generieren von Datenpunkten durch zufällige, gitterbasierte oder Latin-Hypercube-Abtastung werden definiert. Die Funktion generateDataPoints wird verwendet, um Trainingsdaten unter Verwendung der ausgewählten Methode zu generieren. Bei Dimension 2 werden die generierten Datenpunkte in einem Streudiagramm dargestellt.

Aufteilung von Trainings- und Testdaten: Die generierten Daten werden in Trainings- und Testdaten aufgeteilt, wobei der angegebene Trainings-Test-Split (trainTestSplit) verwendet wird.

Erstellen eines RNN-Modells: Eine Funktion zur Erstellung eines sequentiellen neuronalen Netzwerkmodells (RNN) wird definiert. Das Modell besteht aus mehreren Schichten von dichten Neuronen. Das Modell wird mit den Trainingsdaten trainiert und die Trainingszeit wird aufgezeichnet.
Der Aufbau des RNN-Modells ist wie folgt:

Eingangsschicht: layer_dense(units=128, input_shape=2) mit aktivierender "Leaky ReLU"-Funktion.
Verborgene Schicht: layer_dense(units=32) mit aktivierender "Leaky ReLU"-Funktion.
Verborgene Schicht: layer_dense(units=128) mit aktivierender "Leaky ReLU"-Funktion.
Dropout-Schicht: layer_dropout(rate=0.001) mit einer Auslassrate von 0.001.
Verborgene Schicht: layer_dense(units=64) mit aktivierender "Leaky ReLU"-Funktion.
Ausgangsschicht: layer_dense(units=1, activation="linear") für die lineare Ausgabe.
Das Modell verwendet den Mean Squared Logarithmic Error als Verlustfunktion und den Adam-Optimizer für das Training.

Erstellen eines Regressionsmodells (Gaußprozess): Ein Gaußprozess-Regressionsmodell wird unter Verwendung der Funktion kernlab::gausspr erstellt. Die Trainingszeit wird gemessen und aufgezeichnet.

Generieren der Grundwahrheit: Eine Funktion wird erstellt, die die tatsächlichen Funktionswerte basierend auf den Trainingsdaten und der Benchmark-Testfunktion generiert.

Der Versuchsaufbau nutzt maschinelles Lernen, um Optimierungsalgorithmen auf Benchmark-Testfunktionen anzuwenden. Durch das Trainieren von RNN-Modellen und Regressionsmodellen werden die besten Parameter für die gegebenen Testfunktionen ermittelt. Die Generierung der Grundwahrheit hilft dabei, die Qualität der erzielten Modelle zu bewerten. 

------------------------------------------------------------------------



## Phase 3 Optimierung mit Differential Evolution
Theorie:
Die differentielle Evolution (DE) ist ein stochastischer Optimierungsalgorithmus mit realen Parametern, der häufig zur Lösung von Optimierungsproblemen eingesetzt wird[1]. Er arbeitet mit ähnlichen Rechenschritten wie ein standardmäßiger evolutionärer Algorithmus (EA), weist jedoch einige wichtige Unterschiede auf[1]. Hier sind einige wichtige Punkte zur Differentiellen Evolution.
- Störung der Population: DE stört die Populationsmitglieder der aktuellen Generation, indem sie die skalierten Unterschiede zwischen zufällig ausgewählten und unterschiedlichen Populationsmitgliedern verwendet[1]. Dieser Perturbationsschritt hilft, den Suchraum zu erkunden und optimale Lösungen zu finden.
- Keine gesonderte Wahrscheinlichkeitsverteilung: Im Gegensatz zu traditionellen EAs benötigt DE keine separate Wahrscheinlichkeitsverteilung für die Erzeugung von Nachkommen[1]. Die Nachkommen werden auf der Grundlage der gestörten Populationsmitglieder erzeugt.
- Varianten und Verbesserungen: Seit seiner Einführung im Jahr 1995 hat DE die Aufmerksamkeit von Forschern weltweit auf sich gezogen, was zur Entwicklung zahlreicher Varianten mit verbesserter Leistung geführt hat[1]. Diese Varianten beinhalten verschiedene Strategien für die Störung der Population und die Anpassung der Kontrollparameter.
- Anwendungen: DE wurde erfolgreich auf verschiedene Optimierungsprobleme angewandt, einschließlich mehrzieliger, eingeschränkter, großräumiger und unsicherer Optimierungsprobleme[1]. Es hat auch Anwendungen in der Technik gefunden, wo seine leistungsstarke Natur genutzt wurde, um komplexe Optimierungsprobleme zu lösen[1].

Citations:
[1] https://www.semanticscholar.org/paper/b630e7344ebd9d7e1386cb08c9e9fc03c910380c
[2] https://www.semanticscholar.org/paper/4cfc45c26f99078ee25aed24badc69d0ce51eb63
[3] https://www.semanticscholar.org/paper/4b7595bc5e1b8b04c249c3272b0566d4432faf2c

 
Experiment-Protokollierung:
Die Funktion logExperiment führt das DE-Experiment für verschiedene Modelle durch und protokolliert die Ergebnisse. Dabei werden die Grundwahrheit, das RNN-Modell und das Gaußprozess-Modell verglichen.

Benchmarks für verschiedene Populationen:
Die Experimente werden für verschiedene Populationsgrößen (popSize) durchgeführt: 4, 20 und 40 mal die Dimension. Die Ergebnisse werden in log1, log2 und log3 gespeichert.

Chancen und Gründe für eine kleine Populationsgröße (popSize = 4):

Schnellere Ausführung: Eine kleinere Populationsgröße erfordert weniger Berechnungen, was zu einer schnelleren Durchführung der Optimierung führt. Dies kann vorteilhaft sein, wenn die Zeit für die Durchführung der Optimierung begrenzt ist.
Exploration: Eine kleinere Populationsgröße ermöglicht eine schnellere Exploration des Suchraums. Dies kann dazu führen, dass der Algorithmus eine breitere Palette von Lösungen in kürzerer Zeit erkundet.
Chancen und Gründe für eine moderate Populationsgröße (popSize = 20):

Ausgewogenheit zwischen Exploration und Exploitation: Eine moderate Populationsgröße ermöglicht sowohl die Exploration des Suchraums als auch eine bessere Ausnutzung bereits entdeckter vielversprechender Bereiche. Es besteht eine gute Balance zwischen der Breite der Suche und der Vertiefung in vielversprechende Regionen.
Chancen und Gründe für eine größere Populationsgröße (popSize = 40 mal die Dimension):

Bessere Konvergenz: Eine größere Populationsgröße kann dazu beitragen, dass der Algorithmus schneller gegen Ende der Optimierung konvergiert. Dadurch besteht die Möglichkeit, in den letzten Iterationen genauere und genauere Lösungen zu finden.
Höhere Genauigkeit: Eine größere Populationsgröße ermöglicht eine bessere Schätzung der Parameter und Funktionen, was zu einer höheren Genauigkeit der gefundenen Lösungen führen kann.

Zusammenfassend kann man sagen, dass ein Ansatz mit popSize = 4 einige Gemeinsamkeiten mit gradientenbasierten Ansätzen aufweisen kann, insbesondere wenn es darum geht, lokal optimale Lösungen zu finden. Dennoch sind evolutionäre Algorithmen in der Regel weniger anfällig für das Steckenbleiben in lokalen Minima und können eine breitere Exploration des Suchraums ermöglichen. 

Auswertung der Benchmarks und Plots:
Die Funktion plotBenchmark generiert verschiedene Benchmark-Plots, um die Leistung der Modelle zu vergleichen. Die Funktion plotBenchmarkPerformance wird verwendet, um die Performance der Modelle hinsichtlich der Funkevals darzustellen. Die Funktion plotBenchmarkValidation zeigt die Validierung der Modelle anhand der Funkevals.

Ausgabe der Ergebnisse:
Die Ergebnisse der Optimierungsexperimente werden ausgegeben. Dies umfasst sowohl die Benchmark-Plots als auch die Zeiten, die für die Optimierung mit den verschiedenen Modellen benötigt wurden.



|-> Abbruchkriterium auf zu hohem Wert, bzw. wird zu schnell erreicht
|-> 






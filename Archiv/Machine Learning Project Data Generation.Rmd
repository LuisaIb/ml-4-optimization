---
title: "Machine Learning Project"
author: "Hammerer, Ibele, Janez, Romer, Steinwender"
date: "2023-07-29"
output:
  ioslides_presentation: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## requirements

```{r}
require(smoof)
require(ggplot2)
require(keras)
require(SPOT)
require(pracma)
require(lhs)
```





## loading the functions

```{r}
set.seed(1)

loadFunction <- function(num, dim=2) {
  f <- makeBBOBFunction(dim,num,1)
  return(f)
}


plotFunction <- function(f,lower,upper,vectorized=FALSE){
  x <- seq(from=lower[1],to=upper[1],length.out=100)
  y <- seq(from=lower[2],to=upper[2],length.out=100)
  df <-  expand.grid(x = x, y = y)
  if(vectorized)
    z <- f(df)
  else
    z <- apply(df,1,f)
  #z <- log10(z-min(z)+1)
  df$z <- z
  p <- ggplot(df, aes(x, y, z=z))
  p <- p + geom_contour_filled()
  #p <- p + geom_raster(aes(fill=z))
  #p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
  p
}

```

```{r}

f1 <- loadFunction(num=1)
f2 <- loadFunction(num=3)
f3 <- loadFunction(num=23)
f4 <- loadFunction(num=24)



plotFunction(f1,getLowerBoxConstraints(f1),getUpperBoxConstraints(f1))
plotFunction(f2,getLowerBoxConstraints(f2),getUpperBoxConstraints(f2))
plotFunction(f3,getLowerBoxConstraints(f3),getUpperBoxConstraints(f3))
plotFunction(f4,getLowerBoxConstraints(f4),getUpperBoxConstraints(f4))


```

## generating the data

It is not clear how many data points are needed. There must be enough data points to generate a model that is able to fit the data good enough but not too many data points to slow down the algorithm. The data can be generated randomly or with the help of a model.

```{r}

generateRandom <- function(n = 50, lower, upper, dim = 2){
  x <- runif(n,lower[1],upper[1])
  for (i in 2:dim) {
    x <- cbind(x, runif(n,lower[i],upper[i]))
  }
  x
}


generateGrid <- function(n = 50, lower, upper, dim = 2){
  grid_elements <- round(pracma::nthroot(n, dim))
  seq <- NULL
  for (i in 1:dim) {
    seq <- cbind(seq, seq(lower[i], upper[i], length.out=grid_elements))
  }
  if(dim == 2){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2])
  } else if(dim == 3){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3])
  } else if(dim == 4){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3], x1 = seq[,4])
  }
  data
}



generateLHS <- function(n = 50, lower, upper, dim = 2){
  x <- randomLHS(n, dim)
  for(i in 1:dim){
    x[,i] <- lower + (upper-lower)*x[,i]
  }
  x
}


generateDataPoints <- function(n = 50, f, dim = 2, method = "random"){
  ftest <- f 
  lower <- getLowerBoxConstraints(ftest)
  upper <- getUpperBoxConstraints(ftest)
  if (method == "random") {
    x <- generateRandom(n = n, lower = lower, upper = upper, dim = dim)
  } else if (method == "grid") {
    x <- generateGrid(n = n, lower = lower, upper = upper, dim = dim)
  } else if (method == "lhs") {
    x <- generateLHS(n = n, lower = lower, upper = upper, dim = dim)
  } else {
    stop("wrong method - please choose 'random' or 'lhs'")
  }
  df <-  data.frame(x)
  df$z <- apply(df,1,ftest)
  return(df)
}


plot2DDataPoints <- function(df){
  dfPlot <- data.frame(x=df[, 1],y=df[, 2],z=df[, 3])
  ggplot(data=dfPlot,aes(x=x,y=y,colour=z)) +
    geom_point() +
    scale_colour_gradientn(colours=rainbow(4))
}


```

```{r}

df_random <- generateDataPoints(n = 200, f1, dim = 2, method = "random")

plot2DDataPoints(df_random)


df_grid <- generateDataPoints(n = 200, f1, dim = 2, method = "grid")

plot2DDataPoints(df_grid)


df_lhs <- generateDataPoints(n = 200, f1, dim = 2, method = "lhs")

plot2DDataPoints(df_lhs)


```


```{r}

custom_loss <- function(y_true, y_pred) {
  # Implement your custom loss calculation here
  # For example, you can use mean squared error with a weighting factor
  weight_factor <- 0.001
  mse <- mean((y_true - y_pred)^2)
  weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
  return(weighted_mse)
}


# Create a Keras loss function from your custom function
loss_function <- custom_loss

createRNNModel <- function(){
  model = keras_model_sequential() %>% 
   layer_dense(units=128, activation="relu", input_shape=2) %>% 
   layer_activation_leaky_relu() %>% 
   layer_dense(units=32, input_shape=2) %>% 
   layer_activation_leaky_relu() %>% 
   layer_dense(units=128, input_shape=2) %>% 
   layer_activation_leaky_relu() %>%
   layer_dropout(rate=0.01) %>%
   layer_dense(units=64, activation="relu", input_shape=2) %>% 
   layer_dense(units=64, activation="relu", input_shape=2) %>% 
   layer_dense(units=32, activation = "relu") %>% 
   layer_dropout(rate=0.001) %>% 
   layer_dense(units=1, activation="linear")
 
  model %>% compile(
     loss = "mse",
     optimizer =  "adam", 
     metrics = list("mean_absolute_error")
   )
   
  model %>% summary()
  
  model
}


```



```{r}
splitTrainTest <- function(df, dim = 2, tts = 0.8){
  l <- nrow(df)
  s <- l * tts
  X_train <- df[0:s, 1:dim]
  X_train <- data.matrix(X_train)
  y_train <- df[0:s, (dim + 1)]
  X_test <- df[(s+1):l, 1:dim]
  X_test <- data.matrix(X_test)
  y_test <- df[(s+1):l, (dim + 1)]
  list(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test)
}


data_random <- splitTrainTest(df_random)
X_train_random <- data_random$X_train
y_train_random <- data_random$y_train
X_test_random <- data_random$X_test
y_test_random <- data_random$y_test


data_grid <- splitTrainTest(df_grid)
X_train_grid <- data_grid$X_train
y_train_grid <- data_grid$y_train
X_test_grid <- data_grid$X_test
y_test_grid <- data_grid$y_test


data_lhs <- splitTrainTest(df_lhs)
X_train_lhs <- data_lhs$X_train
y_train_lhs <- data_lhs$y_train
X_test_lhs <- data_lhs$X_test
y_test_lhs <- data_lhs$y_test



```



```{r}

model_rnn_random <- createRNNModel()

model_rnn_random %>% fit(X_train_random, y_train_random, epochs = 150,verbose = 0)


y_pred = model_rnn_random %>% predict(X_test)



plotModel <- function(f, model){
  lower <- getLowerBoxConstraints(f)
  upper <- getUpperBoxConstraints(f)
  
  x <- seq(from=lower[1],to=upper[1],length.out=100)
  y <- seq(from=lower[2],to=upper[2],length.out=100)
  
  df <-  expand.grid(x = x, y = y)
  
  x_pred <- cbind(df$x, df$y)
  
  z <- model %>% predict(x_pred)
  
  df$z <- z
  
  p <- ggplot(df, aes(x, y, z=z))
  p <- p + geom_contour_filled()

  
  p
  
}


plotModel(f1, model_rnn_random)


```



```{r}

require(kernlab)
fitted_gpr_random <- kernlab::gausspr(X_train_random,y_train_random)

fpred_random <- function(X_test_random){
  predict(fitted_gpr_random, X_test_random)
}

plotFunction(fpred_random,lower,upper,vectorized=T)
```




```{r}



```



```{r}






```


```{r}



```


```{r}


```



```{r}



```



```{r}



```



## optimization

```{r}

```



```{r}


```




```{r}




```



```{r}



```


```{r}


```



```{r}





```

layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dense(units=16, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mae",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 100,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
model %>% fit(X_train, y_train, epochs = 500,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
getCondition <- function(y_true, y_pred) {
abs(y_true - y_pred) <= 5
}
#custom_loss <- function(y_true, y_pred) {
#  weight_factor <- 0.1
#
#  print("test")
#  print(y_true)
#  print(y_pred)
#
#  y_true_num <- k_eval(y_true)
#  y_pred_num <- k_eval(y_pred)
#
#  print("test 2")
#
#  print(y_true_num)
#  print(y_pred_num)
#
#  condition <- abs(y_true_num - y_pred_num) <= 5
#
#  if (condition) {
#    true_error <- (y_true_num - y_pred_num) * weight_factor
#
#  } else {
#    true_error <- (y_true_num - y_pred_num)
#  }
#
#  mse <- mean(true_error^2)
#
#  mse <- k_constant(array(mse), as.constant = TRUE)
#
#  return(mse)
#}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
# Create a Keras loss function from your custom function
loss_function <- custom_loss
model = keras_model_sequential() %>%
layer_dense(units=128, activation="relu", input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dense(units=16, activation = "relu") %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mae",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
getCondition <- function(y_true, y_pred) {
abs(y_true - y_pred) <= 5
}
#custom_loss <- function(y_true, y_pred) {
#  weight_factor <- 0.1
#
#  print("test")
#  print(y_true)
#  print(y_pred)
#
#  y_true_num <- k_eval(y_true)
#  y_pred_num <- k_eval(y_pred)
#
#  print("test 2")
#
#  print(y_true_num)
#  print(y_pred_num)
#
#  condition <- abs(y_true_num - y_pred_num) <= 5
#
#  if (condition) {
#    true_error <- (y_true_num - y_pred_num) * weight_factor
#
#  } else {
#    true_error <- (y_true_num - y_pred_num)
#  }
#
#  mse <- mean(true_error^2)
#
#  mse <- k_constant(array(mse), as.constant = TRUE)
#
#  return(mse)
#}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
# Create a Keras loss function from your custom function
loss_function <- custom_loss
model = keras_model_sequential() %>%
layer_dense(units=128, activation="relu", input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mae",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
getCondition <- function(y_true, y_pred) {
abs(y_true - y_pred) <= 5
}
#custom_loss <- function(y_true, y_pred) {
#  weight_factor <- 0.1
#
#  print("test")
#  print(y_true)
#  print(y_pred)
#
#  y_true_num <- k_eval(y_true)
#  y_pred_num <- k_eval(y_pred)
#
#  print("test 2")
#
#  print(y_true_num)
#  print(y_pred_num)
#
#  condition <- abs(y_true_num - y_pred_num) <= 5
#
#  if (condition) {
#    true_error <- (y_true_num - y_pred_num) * weight_factor
#
#  } else {
#    true_error <- (y_true_num - y_pred_num)
#  }
#
#  mse <- mean(true_error^2)
#
#  mse <- k_constant(array(mse), as.constant = TRUE)
#
#  return(mse)
#}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
# Create a Keras loss function from your custom function
loss_function <- custom_loss
model = keras_model_sequential() %>%
layer_dense(units=128, activation="relu", input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.001) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mae",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
getCondition <- function(y_true, y_pred) {
abs(y_true - y_pred) <= 5
}
#custom_loss <- function(y_true, y_pred) {
#  weight_factor <- 0.1
#
#  print("test")
#  print(y_true)
#  print(y_pred)
#
#  y_true_num <- k_eval(y_true)
#  y_pred_num <- k_eval(y_pred)
#
#  print("test 2")
#
#  print(y_true_num)
#  print(y_pred_num)
#
#  condition <- abs(y_true_num - y_pred_num) <= 5
#
#  if (condition) {
#    true_error <- (y_true_num - y_pred_num) * weight_factor
#
#  } else {
#    true_error <- (y_true_num - y_pred_num)
#  }
#
#  mse <- mean(true_error^2)
#
#  mse <- k_constant(array(mse), as.constant = TRUE)
#
#  return(mse)
#}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
# Create a Keras loss function from your custom function
loss_function <- custom_loss
model = keras_model_sequential() %>%
layer_dense(units=128, activation="relu", input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.001) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mse",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
#library(reticulate)
#x <- py_eval("1 + 1", convert = FALSE)
#rstudioapi::restartSession()
knitr::opts_chunk$set(echo = TRUE)
require(smoof)
require(ggplot2)
require(keras)
require(SPOT)
set.seed(1)
plotFunction <- function(f,lower,upper,vectorized=FALSE){
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
if(vectorized)
z <- f(df)
else
z <- apply(df,1,f)
#z <- log10(z-min(z)+1)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
}
loadFunction <- function(num, dim=2) {
f <- makeBBOBFunction(dim,num,1)
return(f)
}
f1 <- loadFunction(num=1)
f2 <- loadFunction(num=3)
f3 <- loadFunction(num=23)
f4 <- loadFunction(num=24)
plotFunction(f1,getLowerBoxConstraints(f1),getUpperBoxConstraints(f1))
plotFunction(f2,getLowerBoxConstraints(f2),getUpperBoxConstraints(f2))
plotFunction(f3,getLowerBoxConstraints(f3),getUpperBoxConstraints(f3))
plotFunction(f4,getLowerBoxConstraints(f4),getUpperBoxConstraints(f4))
generateDataPoints <- function(n = 50, f){
ftest <- f #selected instance for plotting / testing
lower <- getLowerBoxConstraints(ftest)
upper <- getUpperBoxConstraints(ftest)
x <- runif(n,lower[1],upper[1])
y <- runif(n,lower[2],upper[2])
df <-  data.frame(x = x, y = y)
df$z <- apply(df,1,ftest)
return(df)
}
plotDataPoints <- function(df){
ggplot(data=df,aes(x=x,y=y,colour=z)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
dfF1 <- generateDataPoints(n = 200, f = f4)
dfF2 <- generateDataPoints(n = 50, f = f4)
plotDataPoints(dfF1)
plotDataPoints(dfF2)
dfF1
if (tensorflow::tf$executing_eagerly())
tensorflow::tf$compat$v1$disable_eager_execution()
X_train <- cbind(dfF1$x, dfF1$y)
y_train <- dfF1$z
X_test <- cbind(dfF2$x, dfF2$y)
y_test <- dfF2$z
getCondition <- function(y_true, y_pred) {
abs(y_true - y_pred) <= 5
}
#custom_loss <- function(y_true, y_pred) {
#  weight_factor <- 0.1
#
#  print("test")
#  print(y_true)
#  print(y_pred)
#
#  y_true_num <- k_eval(y_true)
#  y_pred_num <- k_eval(y_pred)
#
#  print("test 2")
#
#  print(y_true_num)
#  print(y_pred_num)
#
#  condition <- abs(y_true_num - y_pred_num) <= 5
#
#  if (condition) {
#    true_error <- (y_true_num - y_pred_num) * weight_factor
#
#  } else {
#    true_error <- (y_true_num - y_pred_num)
#  }
#
#  mse <- mean(true_error^2)
#
#  mse <- k_constant(array(mse), as.constant = TRUE)
#
#  return(mse)
#}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
# Create a Keras loss function from your custom function
loss_function <- custom_loss
model = keras_model_sequential() %>%
layer_dense(units=128, activation="relu", input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.001) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mse",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
require(kernlab)
fitted_gpr <- kernlab::gausspr(X_train,y_train)
fpred <- function(X_test){
predict(fitted_gpr, X_test)
}
plotFunction(fpred,lower,upper,vectorized=T)
y_pred = model %>% predict(X_test)
x_axes = seq(1:length(y_pred))
plot(x_axes, y_test, type="l", col="red")
lines(x_axes, y_pred, col="blue")
legend("topleft", legend=c("y-original", "y-predicted"),
col=c("red", "blue"), lty=1,cex=0.8)
library(ggplot2)
#library(reshape2)
df <- data.frame(x1=dfF2$x, x2=dfF2$y, y=dfF2$z, y_pred); df
plotDataPoints <- function(df, row){
ggplot(data=df,aes(x=x1,y=x2, colour=row)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
plotDataPoints(df, df$y)
plotDataPoints(df, df$y_pred)
lower <- getLowerBoxConstraints(f4)
upper <- getUpperBoxConstraints(f4)
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
data_approxfum <- approxfun(x, y)
data_approxfum
library(cooltools)
install.packages("cooltools")
library(cooltools)
library(cooltools)
f = approxfun2(dfF1$x,dfF1$y,dfF1$z)

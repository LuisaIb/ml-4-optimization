layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.001) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = loss_huber,
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
y_pred = model %>% predict(X_test)
x_axes = seq(1:length(y_pred))
plot(x_axes, y_test, type="l", col="red")
lines(x_axes, y_pred, col="blue")
legend("topleft", legend=c("y-original", "y-predicted"),
col=c("red", "blue"), lty=1,cex=0.8)
library(ggplot2)
#library(reshape2)
df <- data.frame(x1=dfF2$x, x2=dfF2$y, y=dfF2$z, y_pred); df
plotDataPoints <- function(df, row){
ggplot(data=df,aes(x=x1,y=x2, colour=row)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
plotDataPoints(df, df$y)
plotDataPoints(df, df$y_pred)
lower <- getLowerBoxConstraints(f4)
upper <- getUpperBoxConstraints(f4)
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
#custom_loss <- function(y_true, y_pred) {
#  weight_factor <- 0.1
#
#  print("test")
#  print(y_true)
#  print(y_pred)
#
#  y_true_num <- k_eval(y_true)
#  y_pred_num <- k_eval(y_pred)
#
#  print("test 2")
#
#  print(y_true_num)
#  print(y_pred_num)
#
#  condition <- abs(y_true_num - y_pred_num) <= 5
#
#  if (condition) {
#    true_error <- (y_true_num - y_pred_num) * weight_factor
#
#  } else {
#    true_error <- (y_true_num - y_pred_num)
#  }
#
#  mse <- mean(true_error^2)
#
#  mse <- k_constant(array(mse), as.constant = TRUE)
#
#  return(mse)
#}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
# Create a Keras loss function from your custom function
loss_function <- custom_loss
model = keras_model_sequential() %>%
layer_dense(units=128, activation="relu", input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=32, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=64, activation="relu", input_shape=2) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.001) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
#loss = loss_huber,
#loss = loss_cosine_similarity,
loss = loss_logcosh,
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(X_train, y_train, epochs = 150,verbose = 0)
scores = model %>% evaluate(X_train, y_train, verbose = 0)
print(scores)
scores = model %>% evaluate(X_test, y_test, verbose = 0)
print(scores)
y_pred = model %>% predict(X_test)
x_axes = seq(1:length(y_pred))
plot(x_axes, y_test, type="l", col="red")
lines(x_axes, y_pred, col="blue")
legend("topleft", legend=c("y-original", "y-predicted"),
col=c("red", "blue"), lty=1,cex=0.8)
plotDataPoints <- function(df, row){
ggplot(data=df,aes(x=x1,y=x2, colour=row)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
lower <- getLowerBoxConstraints(f4)
z <- model %>% predict(x_pred)
z <- model %>% predict(x_pred)
df$z <- z
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
knitr::opts_chunk$set(echo = TRUE)
require(smoof)
require(ggplot2)
require(keras)
require(SPOT)
set.seed(1)
plotFunction <- function(f,lower,upper,vectorized=FALSE){
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
if(vectorized)
z <- f(df)
else
z <- apply(df,1,f)
#z <- log10(z-min(z)+1)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
#p <- p + geom_raster(aes(fill=z))
#p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
p
}
loadFunction <- function(num, dim=2) {
f <- makeBBOBFunction(dim,num,1)
return(f)
}
f1 <- loadFunction(num=1)
f2 <- loadFunction(num=3)
f3 <- loadFunction(num=23)
f4 <- loadFunction(num=24)
plotFunction(f1,getLowerBoxConstraints(f1),getUpperBoxConstraints(f1))
plotFunction(f2,getLowerBoxConstraints(f2),getUpperBoxConstraints(f2))
plotFunction(f3,getLowerBoxConstraints(f3),getUpperBoxConstraints(f3))
plotFunction(f4,getLowerBoxConstraints(f4),getUpperBoxConstraints(f4))
generateDataPoints <- function(n = 50, f){
ftest <- f #selected instance for plotting / testing
lower <- getLowerBoxConstraints(ftest)
upper <- getUpperBoxConstraints(ftest)
x <- runif(n,lower[1],upper[1])
y <- runif(n,lower[2],upper[2])
df <-  data.frame(x = x, y = y)
df$z <- apply(df,1,ftest)
return(df)
}
plotDataPoints <- function(df){
ggplot(data=df,aes(x=x,y=y,colour=z)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
dfF1 <- generateDataPoints(n = 1000, f = f1)
dfF2 <- generateDataPoints(n = 100, f = f1)
plotDataPoints(dfF1)
plotDataPoints(dfF2)
dfF1
if (tensorflow::tf$executing_eagerly())
tensorflow::tf$compat$v1$disable_eager_execution()
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
lower
upper
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
lower
upper
f1(upper[1])
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
lower
upper
f1(upper)
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
lower
upper
f1(lower)
# Visualizations (Optional, for 2D latent space visualization)
# You may need to modify this part according to your specific regression task.
# Plot the 2D latent space
x_test_encoded <- predict(encoder, input_data, batch_size = batch_size)
scaleData <- function(df){
min <- NULL
max <- NULL
for(i in 1:ncol(df)) {
min <- cbind(min, min(df[ , i]))
max <- cbind(max, max(df[ , i]))
df[ , i] <- round((df[ , i] - min(df[ , i])) / (max(df[ , i]) - min(df[ , i])), 7)
}
list(df=df, min=min, max=max)
}
descaleData <- function(df, min, max){
for(i in 1:ncol(df)){
df[ , i] <- df[ , i] * (max[i] - min[i]) + min[i]
}
df
}
# Parameters
batch_size <- 64L  #64
input_dim <- 3L  # Dimension of the input data
latent_dim <- 5L
intermediate_dim <- 1024L
epochs <- 100L
epsilon_std <- 1.0
# Model definition
# input layer; defining shape of input
x <- layer_input(shape = c(input_dim))
# dense layer, amount of neurons is the number of variable intermediate_dim, activation function is relu
h <- layer_dense(x, intermediate_dim, activation = "relu")
# dense layer, amount of neurons is the number of variable intermediate_dim, activation function is relu
i <- layer_dense(h, intermediate_dim, activation = "relu")
# dense layer, amount of neurons is the number of variable latent_dim
z_mean <- layer_dense(i, latent_dim)
# dense layer, amount of neurons is the number of variable latent_dim
z_log_var <- layer_dense(i, latent_dim)
# function that
sampling <- function(arg) {
# z_mean filled with all rows and columns 1 til 5 from input data arg
z_mean <- arg[, 1:(latent_dim)]
# z_log_var filled with all rows and columns 6 bis 10 from input data arg
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
# epsilon filled with normalization function
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean = 0.,
stddev = epsilon_std
)
# return value
z_mean + k_exp(z_log_var/2) * epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
# combining the layers z_mean and z_log_war
# using sampling method in the lambda layer
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
# dense layer, amount of neurons is the number of variable intermediate_dim, activation function is relu
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
# Use 'linear' activation for regression
decoder_mean <- layer_dense(units = input_dim, activation = "linear")
# layer decoder_h used on layer z
h_decoded <- decoder_h(z)
# layer decoder_mean used on layer h_decoded
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
# input layer with shape of latent_dim
decoder_input <- layer_input(shape = latent_dim)
# using layer decoder_input
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
vae_loss <- function(x, x_decoded_mean) {
# Use Mean Squared Error for regression
mse_loss <- loss_mean_squared_error(x, x_decoded_mean)
kl_loss <- -0.5 * k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
mse_loss + kl_loss
}
vae %>% compile(optimizer=optimizer_adam(lr = 0.0001), loss = vae_loss, )
# Custom Data preparation
# Assuming you have your custom data (x_data, y_data, z_data) and target values (target_values)
# Combine the x, y, z data to form the complete input data
input_data <- cbind(dfF1$x, dfF1$y, dfF1$z)
test_data <- cbind(dfF2$x, dfF2$y, dfF2$z)
#input_scaled <- scaleData(input_data)
#test_scaled <- scaleData(test_data)
#input_data_scaled <- input_scaled$df
#test_data_scaled <- test_scaled$df
#target_values <- dfF1$z
# Normalize the input data (optional but recommended for better convergence)
#input_data <- scale(input_data)
#input_data <- scale(input_data,, center = TRUE, scale = TRUE
# Model training
vae %>% fit(
input_data, input_data,  # Use your custom input data and target values
#shuffle = TRUE,
epochs = epochs,
batch_size = batch_size,
validation_data = list(test_data, test_data)
)
# Visualizations (Optional, for 2D latent space visualization)
# You may need to modify this part according to your specific regression task.
# Plot the 2D latent space
x_test_encoded <- predict(encoder, input_data, batch_size = batch_size)
x_test_decoded <- predict(generator, x_test_encoded, batch_size = batch_size)
x_test_decoded
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
x_test_decoded_x <- x_test_decoded[,1:2]
x_test_decoded_x
x_test_decoded_descaled <- descaleData(x_test_decoded_x, lower, upper)
x_test_decoded_descaled
# Speichere die Mittelwerte und Standardabweichungen der Eingabedaten
#input_mean <- attr(input_data, "scaled:center")
#input_sd <- attr(input_data, "scaled:scale")
# Wende die umgekehrte Normalisierung auf die dekodierten Daten an
#x_test_decoded_original <- x_test_decoded * input_sd + input_mean
#x_test_decoded_original
library(ggplot2)
library(dplyr)
#x_test_encoded %>%
#  as_data_frame() %>%
#  ggplot(aes(x = V1, y = V2)) + geom_point()
# Access the training and test losses from the history object
#train_loss <- history$metrics$loss
#test_loss <- history$metrics$val_loss
# Print the training and test losses for each epoch
#for (epoch in 1:epochs) {
#  print(paste("Epoch:", epoch, "Training Loss:", train_loss[[epoch]], "Test Loss:", #test_loss[[epoch]]))
#}
# Display a 2D manifold of the reconstructed data
#rows <- NULL
#for (i in 1:length(grid_x)) {
#  column <- NULL
#  for (j in 1:length(grid_y)) {
#    for (k in 1:length(grid_z)) {
#      z_sample <- matrix(c(grid_x[i], grid_y[j], grid_z[k]), ncol = 3)
#      column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = input_dim))
#    }
#  }
#  rows <- cbind(rows, column)
#}
#rows %>% as.data.frame() %>% plot()
# Visualizations (Optional, for 2D latent space visualization)
# You may need to modify this part according to your specific regression task.
# Plot the 2D latent space
x_test_encoded <- predict(encoder, input_data, batch_size = batch_size)
x_test_decoded <- predict(generator, x_test_encoded, batch_size = batch_size)
#x_test_decoded
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
x_test_decoded_x <- x_test_decoded[,1:2]
#x_test_decoded_x
scaled <- scaleData(x_test_decoded_x)
x_test_decoded_x <- scaled$df
x_test_decoded_descaled <- descaleData(x_test_decoded_x, lower, upper)
x_test_decoded_descaled
# Speichere die Mittelwerte und Standardabweichungen der Eingabedaten
#input_mean <- attr(input_data, "scaled:center")
#input_sd <- attr(input_data, "scaled:scale")
# Wende die umgekehrte Normalisierung auf die dekodierten Daten an
#x_test_decoded_original <- x_test_decoded * input_sd + input_mean
#x_test_decoded_original
library(ggplot2)
library(dplyr)
#x_test_encoded %>%
#  as_data_frame() %>%
#  ggplot(aes(x = V1, y = V2)) + geom_point()
# Access the training and test losses from the history object
#train_loss <- history$metrics$loss
#test_loss <- history$metrics$val_loss
# Print the training and test losses for each epoch
#for (epoch in 1:epochs) {
#  print(paste("Epoch:", epoch, "Training Loss:", train_loss[[epoch]], "Test Loss:", #test_loss[[epoch]]))
#}
# Display a 2D manifold of the reconstructed data
#rows <- NULL
#for (i in 1:length(grid_x)) {
#  column <- NULL
#  for (j in 1:length(grid_y)) {
#    for (k in 1:length(grid_z)) {
#      z_sample <- matrix(c(grid_x[i], grid_y[j], grid_z[k]), ncol = 3)
#      column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = input_dim))
#    }
#  }
#  rows <- cbind(rows, column)
#}
#rows %>% as.data.frame() %>% plot()
# Visualizations (Optional, for 2D latent space visualization)
# You may need to modify this part according to your specific regression task.
# Plot the 2D latent space
x_test_encoded <- predict(encoder, input_data, batch_size = batch_size)
x_test_decoded <- predict(generator, x_test_encoded, batch_size = batch_size)
#x_test_decoded
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
x_test_decoded_x <- x_test_decoded[,1:2]
#x_test_decoded_x
scaled <- scaleData(x_test_decoded_x)
x_test_decoded_x <- scaled$df
x_test_decoded_descaled <- descaleData(x_test_decoded_x, lower, upper)
range(x_test_decoded_descaled)
# Speichere die Mittelwerte und Standardabweichungen der Eingabedaten
#input_mean <- attr(input_data, "scaled:center")
#input_sd <- attr(input_data, "scaled:scale")
# Wende die umgekehrte Normalisierung auf die dekodierten Daten an
#x_test_decoded_original <- x_test_decoded * input_sd + input_mean
#x_test_decoded_original
library(ggplot2)
library(dplyr)
#x_test_encoded %>%
#  as_data_frame() %>%
#  ggplot(aes(x = V1, y = V2)) + geom_point()
# Access the training and test losses from the history object
#train_loss <- history$metrics$loss
#test_loss <- history$metrics$val_loss
# Print the training and test losses for each epoch
#for (epoch in 1:epochs) {
#  print(paste("Epoch:", epoch, "Training Loss:", train_loss[[epoch]], "Test Loss:", #test_loss[[epoch]]))
#}
# Display a 2D manifold of the reconstructed data
#rows <- NULL
#for (i in 1:length(grid_x)) {
#  column <- NULL
#  for (j in 1:length(grid_y)) {
#    for (k in 1:length(grid_z)) {
#      z_sample <- matrix(c(grid_x[i], grid_y[j], grid_z[k]), ncol = 3)
#      column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = input_dim))
#    }
#  }
#  rows <- cbind(rows, column)
#}
#rows %>% as.data.frame() %>% plot()
# Visualizations (Optional, for 2D latent space visualization)
# You may need to modify this part according to your specific regression task.
# Plot the 2D latent space
x_test_encoded <- predict(encoder, input_data, batch_size = batch_size)
x_test_decoded <- predict(generator, x_test_encoded, batch_size = batch_size)
#x_test_decoded
lower <- getLowerBoxConstraints(f1)
upper <- getUpperBoxConstraints(f1)
x_test_decoded_x <- x_test_decoded[,1:2]
#x_test_decoded_x
scaled <- scaleData(x_test_decoded_x)
x_test_decoded_x <- scaled$df
x_test_decoded_descaled <- descaleData(x_test_decoded_x, lower, upper)
range(x_test_decoded_descaled)
range(x_test_decoded[,3])
# Speichere die Mittelwerte und Standardabweichungen der Eingabedaten
#input_mean <- attr(input_data, "scaled:center")
#input_sd <- attr(input_data, "scaled:scale")
# Wende die umgekehrte Normalisierung auf die dekodierten Daten an
#x_test_decoded_original <- x_test_decoded * input_sd + input_mean
#x_test_decoded_original
library(ggplot2)
library(dplyr)
#x_test_encoded %>%
#  as_data_frame() %>%
#  ggplot(aes(x = V1, y = V2)) + geom_point()
# Access the training and test losses from the history object
#train_loss <- history$metrics$loss
#test_loss <- history$metrics$val_loss
# Print the training and test losses for each epoch
#for (epoch in 1:epochs) {
#  print(paste("Epoch:", epoch, "Training Loss:", train_loss[[epoch]], "Test Loss:", #test_loss[[epoch]]))
#}
# Display a 2D manifold of the reconstructed data
#rows <- NULL
#for (i in 1:length(grid_x)) {
#  column <- NULL
#  for (j in 1:length(grid_y)) {
#    for (k in 1:length(grid_z)) {
#      z_sample <- matrix(c(grid_x[i], grid_y[j], grid_z[k]), ncol = 3)
#      column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = input_dim))
#    }
#  }
#  rows <- cbind(rows, column)
#}
#rows %>% as.data.frame() %>% plot()
x_test_decoded_descaled
x_test_decoded_descaled[,1]
library(ggplot2)
#library(reshape2)
df <- data.frame(x1=x_test_decoded_descaled[,1], x2=x_test_decoded_descaled[,2], y=x_test_decoded[,3]); df
plotDataPoints <- function(df, row){
ggplot(data=df,aes(x=x1,y=x2, colour=row)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
plotDataPoints(df, df$y)

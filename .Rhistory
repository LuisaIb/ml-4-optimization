## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
resgt <- loggedExperiment(exprLBFGSB, groundtruth, 1:10,10)
resgt
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0,factr=-Inf)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control$populationSize <- NULL
control$funEvals <- NULL
control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
# from the estimation of cobbs result
reses <- loggedExperiment(exprLBFGSB, cobbsResult$estimation, 1:10,10)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control$populationSize <- NULL
control$funEvals <- NULL
control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=list(control,factr=-INF),...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
# from the estimation of cobbs result
reses <- loggedExperiment(exprLBFGSB, cobbsResult$estimation, 1:10,10)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control$populationSize <- NULL
control$funEvals <- NULL
control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=list(control,factr=-Inf),...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
# from the estimation of cobbs result
reses <- loggedExperiment(exprLBFGSB, cobbsResult$estimation, 1:10,10)
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20,factr=-Inf))
)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control$populationSize <- NULL
control$funEvals <- NULL
control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20,factr=-Inf))
)
# from the estimation of cobbs result
reses <- loggedExperiment(exprLBFGSB, cobbsResult$estimation, 1:10,10)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0,factr=-Inf)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control$populationSize <- NULL
control$funEvals <- NULL
control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20,factr=-Inf))
)
# from the estimation of cobbs result
reses <- loggedExperiment(exprLBFGSB, cobbsResult$estimation, 1:10,10)
knitr::opts_chunk$set(echo = TRUE)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0,factr=-Inf)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control <- list(funEvals=200,
populationSize= (10 * length(lower)),
trace=0,
factr=-Inf
)
#control$populationSize <- NULL
#control$funEvals <- NULL
#control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
knitr::opts_chunk$set(echo = TRUE)
require(smoof)
require(ggplot2)
require(keras)
require(SPOT)
require(pracma)
require(lhs)
require(cmaes)
require(COBBS)
require(nloptr)
set.seed(1)
numBbobf <- 1
dim <- 2
dataGenerationMethod <- "random" #"grid", "lhs"
numDataPoints <- 1000
trainTestSplit <- 0.8
loadFunction <- function(numBbobf=1, dim=2) {
f <- makeBBOBFunction(dim,numBbobf,1)
return(f)
}
plot2DFunction <- function(f,lower,upper,vectorized=FALSE){
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
if(vectorized)
z <- f(df)
else
z <- apply(df,1,f)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
p
}
bbobf <- loadFunction(num=numBbobf,dim=dim)
lower <- getLowerBoxConstraints(bbobf)
upper <- getUpperBoxConstraints(bbobf)
if(dim==2){
plot2DFunction(bbobf,lower,upper)
} else if (dim==3){
NULL
}
generateRandom <- function(n=50,lower,upper,dim=2){
x <- runif(n,lower[1],upper[1])
for (i in 2:dim) {
x <- cbind(x, runif(n,lower[i],upper[i]))
}
x
}
generateGrid <- function(n = 50, lower, upper, dim = 2){
grid_elements <- round(pracma::nthroot(n, dim))
seq <- NULL
for (i in 1:dim) {
seq <- cbind(seq, seq(lower[i], upper[i], length.out=grid_elements))
}
if(dim == 2){
data <- expand.grid(x1 = seq[,1], x2 = seq[,2])
} else if(dim == 3){
data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3])
} else if(dim == 4){
data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3], x1 = seq[,4])
}
data
}
generateLHS <- function(n = 50, lower, upper, dim = 2){
x <- randomLHS(n, dim)
for(i in 1:dim){
x[,i] <- lower + (upper-lower)*x[,i]
}
x
}
generateDataPoints <- function(n = 50, f, dim = 2, method = "random"){
ftest <- f
lower <- getLowerBoxConstraints(ftest)
upper <- getUpperBoxConstraints(ftest)
if (method == "random") {
x <- generateRandom(n = n, lower = lower, upper = upper, dim = dim)
} else if (method == "grid") {
x <- generateGrid(n = n, lower = lower, upper = upper, dim = dim)
} else if (method == "lhs") {
x <- generateLHS(n = n, lower = lower, upper = upper, dim = dim)
} else {
stop("wrong method - please choose 'random' or 'lhs'")
}
df <-  data.frame(x)
df$z <- apply(df,1,ftest)
return(df)
}
plot2DDataPoints <- function(df){
dfPlot <- data.frame(x=df[, 1],y=df[, 2],z=df[, 3])
ggplot(data=dfPlot,aes(x=x,y=y,colour=z)) +
geom_point() +
scale_colour_gradientn(colours=rainbow(4))
}
df <- generateDataPoints(n = numDataPoints, bbobf, dim = dim, method = dataGenerationMethod)
if(dim==2){
plot2DDataPoints(df)
}
custom_loss <- function(y_true, y_pred) {
# Implement your custom loss calculation here
# For example, you can use mean squared error with a weighting factor
weight_factor <- 0.001
mse <- mean((y_true - y_pred)^2)
weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
return(weighted_mse)
}
createRNNModel <- function(){
model = keras_model_sequential() %>%
layer_dense(units=128, input_shape=2) %>%
layer_activation_leaky_relu() %>%
#   layer_dense(units=32, input_shape=2) %>%
#   layer_activation_leaky_relu() %>%
#   layer_dense(units=128, input_shape=2) %>%
#   layer_activation_leaky_relu() %>%
layer_dropout(rate=0.01) %>%
#   layer_dense(units=64, activation="relu", input_shape=2) %>%
#   layer_dense(units=64, activation="relu", input_shape=2) %>%
#   layer_dense(units=32, activation = "relu") %>%
#   layer_dropout(rate=0.001) %>%
layer_dense(units=1, activation="linear")
model %>% compile(
loss = "mse",
optimizer =  "adam",
metrics = list("mean_absolute_error")
)
model %>% summary()
model
}
splitTrainTest <- function(df, dim = dim, tts = 0.8){
l <- nrow(df)
s <- l * tts
X_train <- df[0:s, 1:dim]
X_train <- data.matrix(X_train)
y_train <- df[0:s, (dim + 1)]
X_test <- df[(s+1):l, 1:dim]
X_test <- data.matrix(X_test)
y_test <- df[(s+1):l, (dim + 1)]
list(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test)
}
data <- splitTrainTest(df, dim, trainTestSplit)
X_train <- data$X_train
y_train <- data$y_train
X_test <- data$X_test
y_test <- data$y_test
model_rnn_random <- createRNNModel()
model_rnn_random %>% fit(X_train_random, y_train_random, epochs = 150,verbose = 0)
y_pred = model_rnn_random %>% predict(X_test)
plotModel <- function(model, lower, upper){
x <- seq(from=lower[1],to=upper[1],length.out=100)
y <- seq(from=lower[2],to=upper[2],length.out=100)
df <-  expand.grid(x = x, y = y)
x_pred <- cbind(df$x, df$y)
z <- model %>% predict(x_pred)
df$z <- z
p <- ggplot(df, aes(x, y, z=z))
p <- p + geom_contour_filled()
p
}
plotModel(model_rnn_random, lower, upper)
require(kernlab)
fitted_gpr_random <- kernlab::gausspr(X_train_random,y_train_random)
plotModel(fitted_gpr_random,lower,upper)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0,factr=-Inf)
con[names(control)] <- control
control<-con
funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control <- list(funEvals=200,
populationSize= (10 * length(lower)),
trace=0,
factr=-Inf
)
#control$populationSize <- NULL
#control$funEvals <- NULL
#control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## specify some model configuration
mc  <- list(useLambda=F,thetaLower=1e-6,thetaUpper=1e12)
## and some configuration details for the simulation
cntrl <- list(modelControl=mc,
nsim=1,
seed=1,
method="spectral",
Ncos = 100,
conditionalSimulation=TRUE
)
## Log results from a single optimization run to generate training data.
exprDE <- expression(
res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20,factr=-Inf))
)
require(COBBS)
resgtde <- loggedExperiment(exprDE, groundtruth, 1,logx = TRUE)
resgtde
resgtde <- resgtde[1:(dimension*50),]
resgtde
x <- as.matrix(resgtde[,c(4,5)]) # training data: features
y <- as.matrix(resgtde[,2,drop=F]) # training data: observations
## generate some ground-truth via BBOB
groundtruth <- function(x){
x=matrix(x,,2)
apply(x,1,bbobf)
}
groundtruth(x)
## generate model and test functions from the data -> generating a model / function
cobbsResult <- generateCOBBS(x,y,cntrl)
cobbsResult$fit
## also: 2-level model
#cobbsFit2 <- COBBS::gaussianProcessR2L(x, y, control = list(useLambda=F))
#cobbsResult2 <- COBBS::simulateFunction(cobbsFit2,seed = 1,nsim=1,Ncos = 100,conditionalSimulation = #T)
# second plot shows the first function of the simulation - not much smoothing - overall quite good
#plotf(cobbsResult2[[1]],lower,upper,vectorized=T)
## prepare an expression that will be run during the experiments
## here: DE
# DEinterface is used for minimization of a function that used the DEoptim algorithm
# → Minimization for Differential Evolution
# "interface" for the differential evolution → any other optimization algotirhm could be used
#exprDE <- expression(
#  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=1000*dimension))
#)
#exprLBFGSB <- expression(
#  res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=1000*dimension))
#)
## run the experiments, with logging
## with each objective function produced by COBBS
# from the ground truth
resgtde <- loggedExperiment(exprDE, groundtruth, 1:10,10)
resgtde
# from the estimation of cobbs result
resesde <- loggedExperiment(exprDE, cobbsResult$estimation, 1:10,10)
resesde
# from the rnn  result
resnnde <- loggedExperiment(exprDE, model_rnn_random$predict, 1:10,10)
resnnde
## plot results
print(plotBenchmarkPerformance(list(resgtde,resesde,resnnde),
c("groundtruth","gaussestimation","nnreg")))
## plot error, comparing against groundtruth
print(plotBenchmarkValidation(resgtde,list(resesde,resnnde),
c("gaussestimation","nnreg")))
resgt <- loggedExperiment(exprLBFGSB, groundtruth, 1:10,10)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
#con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0,factr=-Inf)
#con[names(control)] <- control
#control<-con
#funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control <- list(funEvals=200,
populationSize= (10 * length(lower)),
trace=0,
factr=-Inf
)
#control$populationSize <- NULL
#control$funEvals <- NULL
#control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
resgt <- loggedExperiment(exprLBFGSB, groundtruth, 1:10,10)
LBFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
#con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0,factr=-Inf)
#con[names(control)] <- control
#control<-con
#funEvals <- control$funEvals
#NP <- control$populationSize
## recalculate funEvals to DE iteration on basis of population size
#itermax <- floor((funEvals - NP) / NP)
#if(itermax < 1) itermax= 1
#control$NP <- control$populationSize
#control$itermax <- itermax
#factr <- -Inf
#control$factr <- factr
## Delete unused settings
control <- list(funEvals=200,
populationSize= (10 * length(lower)),
factr=-Inf
)
#control$populationSize <- NULL
#control$funEvals <- NULL
#control$types <- NULL
## wrapper for matrix inputs to fun
fn <- function(x,...)fun(matrix(x,1),...)
## start optim
res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}
## Log results from a single optimization run to generate training data.
exprLBFGSB <- expression(
res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)
resgt <- loggedExperiment(exprLBFGSB, groundtruth, 1:10,10)

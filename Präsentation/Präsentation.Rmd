---
title: "Machine Learning Project"
author: "Hammerer Lena, Ibele Luisa, Janez Isabel, Romer Judith, Steinwender Hanna"
date: "2023-08-09"
output:
  ioslides_presentation:
    widescreen: true
    transition: faster
    highlight: espresso
    css: styles.css
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Titel der ersten Folie

Inhalt der ersten Folie.

## Untertitel 1

Inhalt der ersten Untertitelfolie.

## Untertitel 2

Inhalt der zweiten Untertitelfolie.

# Titel der zweiten Folie

Inhalt der zweiten Folie.

```{r, echo = FALSE}
# Hier wird der Code ausgeblendet
x <- 1:10
x
```

```{r, results='hide'}
# Hier wird der Code ausgeblendet, aber das Ergebnis wird angezeigt
x <- 1:10
x
```

```{r, include=FALSE}
# Hier wird sowohl der Code als auch das Ergebnis ausgeblendet
x <- 1:10
x
```

```{r, eval=FALSE}
# Hier wird der Code ausgeführt, aber weder der Code noch das Ergebnis werden angezeigt
x <- 1:10
x
```

-----------------------------------------------------------------------------------------------------

## Gliederung
- Motivation und Zielsetzung
- Vorgehen und Methodik
- Bewertungsmatrix
- Einführung
- Versuchsaufbau
- Umsetzung und Auswertung
- Fazit
- Kritische Diskussion

# Motivation und Zielsetzung
## Motivation 

## Zielsetzung



# Vorgehen und Methodik
## Vorgehen 

## Methodik



# Bewertungsmatrix
## Modellbewertung
![Alt-Text](Präsentation\modelMatrix.PNG)

## Optimierungsbewertung
![Alt-Text](Präsentation\optimizationMatrix.PNG)

## Finale Bewertungsmatrix
![Alt-Text](Präsentation\generalMatrix.PNG)


# Einführung
## GroundTruth Function

## RNN

## Gauss

## Differential Evolution

## Variantional Autoencoder

## Loss Function


# Versuchsaufbau
## Forschungsfragen
1. Ist der Einsatz eines Variational Autoencoder als Datenerhebungstrategie sinnvoll? 
2. Ist ein RNN besser als ein Gauss bei der Optimierung? 
3. Ist der Einsatz von RNN geeignet?

## Variable Parameter
- numBbobf <- 1/24
- dim <- 2/3
- dataGenerationMethod <- "lhs", "random", "grid"
- numDataPoints <- 25/600
- trainTestSplit <- 0.8
- funEval <- 200/400

## Datenerhebungstrategie
- Random Sampling
- Grid Sampling
- Latin Hypercube Sampling

## Modellaufbau RNN
- Eingangsschicht: layer_dense(units=128, input_shape=2) mit aktivierender "Leaky ReLU"-Funktion.
- Verborgene Schicht: layer_dense(units=32) mit aktivierender "Leaky ReLU"-Funktion.
- Verborgene Schicht: layer_dense(units=128) mit aktivierender "Leaky ReLU"-Funktion.
- Dropout-Schicht: layer_dropout(rate=0.001) mit einer Auslassrate von 0.001.
- Verborgene Schicht: layer_dense(units=64) mit aktivierender "Leaky ReLU"-Funktion.
- Ausgangsschicht: layer_dense(units=1, activation="linear") für die lineare Ausgabe.

Das Modell verwendet den Mean Squared Logarithmic Error als Verlustfunktion und den Adam-Optimizer für das Training.

## Optimierung mit Differential Evolution
- popSize = 4
- popSize = 10*dim
- popSize = 20*dim

## Verworfene Ansätze
- VAE für Regression
- VAE wegen Dimensionsreduktion nicht für Datenerhebung geeignet
- Loss Function selber schreiben
- L-BFGS-B als Optimierer


# Umsetzung und Auswertung
## Übersicht Experimente und Versuche - 1. Experiment
Versuch 1.1 - f1 + 25 Datenpunkte
Versuch 1.1.1 f1 + 25 Datenpunkte + 2 dim 
Versuch 1.1.1.1 f1 + 25 Datenpunkte + 2 dim + Random Sampling + (4, 10*dim, 20*dim)
Versuch 1.1.1.2 f1 + 25 Datenpunkte + 2 dim + Grid Sampling + (4, 10*dim, 20*dim)
Versuch 1.1.1.3 f1 + 25 Datenpunkte + 2 dim + LHS + (4, 10*dim, 20*dim)

Versuch 1.1.2 f1 + 25 Datenpunkte + 3 dim
Versuch 1.1.2.1 f1 + 25 Datenpunkte + 3 dim + Grid Sampling + (popsize = 4)

Versuch 1.2 - f4 + 25 Datenpunkte
Versuch 1.2.1 f4 + 25 Datenpunkte + 2 dim
Versuch 1.2.1.1 f4 + 25 Datenpunkte + 2 dim + Random Sampling + (4, 10*dim, 20*dim)
Versuch 1.2.1.2 f4 + 25 Datenpunkte + 2 dim + Grid Sampling + (4, 10*dim, 20*dim)
Versuch 1.2.1.3 f4 + 25 Datenpunkte + 2 dim + LHS + (4, 10*dim, 20*dim)

Versuch 1.2.2 f4 + 25 Datenpunkte + 3 dim
Versuch 1.2.2.1 f4 + 25 Datenpunkte + 3 dim + Grid Sampling + (popsize = 4)


## Übersicht Experimente und Versuche - 2. Experiment
Versuch 2.1 - f1 + 600 Datenpunkte
Versuch 2.1.1 f1 + 600 Datenpunkte + 2 dim
Versuch 2.1.1.1 f1 + 600 Datenpunkte + 2 dim + Random Sampling + (4, 10*dim, 20*dim)
Versuch 2.1.1.2 f1 + 600 Datenpunkte + 2 dim + Grid Sampling + (4, 10*dim, 20*dim)
Versuch 2.1.1.3 f1 + 600 Datenpunkte + 2 dim + LHS + (4, 10*dim, 20*dim)

Versuch 2.1.2 f1 + 600 Datenpunkte + 3 dim
Versuch 2.1.2.2 f1 + 600 Datenpunkte + 3 dim + Grid Sampling + (popsize = 4)

Versuch 2.2 - f4 + 600 Datenpunkte
Versuch 2.2.1 f4 + 600 Datenpunkte + 2 dim
Versuch 2.2.1.1 f4 + 600 Datenpunkte + 2 dim + Random Sampling + (4, 10*dim, 20*dim)
Versuch 2.2.1.2 f4 + 600 Datenpunkte + 2 dim + Grid Sampling + (4, 10*dim, 20*dim)
Versuch 2.2.1.3 f4 + 600 Datenpunkte + 2 dim + LHS + (4, 10*dim, 20*dim)

Versuch 2.2.2 f4 + 600 Datenpunkte + 3 dim
Versuch 2.2.2.1 f4 + 600 Datenpunkte + 3 dim + Random Sampling + (popsize = 4)


## Übersicht Experimente und Versuche - 3. Experiment
Versuch 3.1 - 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 2 dim
Versuch 3.1.1 f1 + 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 2 dim
Versuch 3.1.2 f4 + 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 2 dim

Versuch 3.2 - 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 3 dim
Versuch 3.2.1 f1 + 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 3 dim
Versuch 3.2.2 f4 + 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 3 dim

Versuch 3.3 - 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 2 dim
Versuch 3.3.1 f1 + 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 2 dim
Versuch 3.3.1 f4 + 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 2 dim

Versuch 3.4 - 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 3 dim
Versuch 3.4.1 f1 + 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 3 dim
Versuch 3.4.1 f4 + 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 3 dim

## Einblicke in die besten Experimentergebnisse
- Versuch 3.1 - 50 Datenpunkte Training + 15 Datenpunkte Test + 300 Epochs + 2 dim
- Versuch 3.3 - 1000 Datenpunkte Training + 200 Datenpunkte Test + 100 Epochs + 2 dim

- Versuch 1.1.2.1 f1 + 25 Datenpunkte + 2 dim + Grid Sampling + (popsize = 4)
- Versuch 1.1.2.1 f1 + 25 Datenpunkte + 3 dim + Grid Sampling + (popsize = 4)

- Versuch 1.2.2.1 f4 + 25 Datenpunkte + 2 dim + Grid Sampling + (popsize = 4)
- Versuch 1.2.2.1 f4 + 25 Datenpunkte + 3 dim + Grid Sampling + (popsize = 4)

- Versuch 2.1.2.2 f1 + 600 Datenpunkte + 2 dim + Grid Sampling + (popsize = 4)
- Versuch 2.1.2.2 f1 + 600 Datenpunkte + 3 dim + Grid Sampling + (popsize = 4)

- Versuch 2.2.2.1 f4 + 600 Datenpunkte + 2 dim + Random Sampling + (popsize = 4)
- Versuch 2.2.2.1 f4 + 600 Datenpunkte + 3 dim + Random Sampling + (popsize = 4)

# Versuch 3.1
## Versuch 3.1.1 

Bild Fuktion ![Alt-Text](images/Versuch%204/GroundtruthF1)

Bild Fuktion zugeschnitten

Bild Datenpunkte zufällig Training
![Alt-Text](images/Versuch%204/DataGenerationRandomTrainF1MinimalData)

Bild Datenpunkte zufällig Test
![Alt-Text](images/Versuch%204/DataGenerationRandomTestF1MinimalData)

Bild Datenpunkte VAE
![Alt-Text](images/Versuch%204/images/Versuch%204/VAEDataF1MinimalData)

Bild Loss Function ![Alt-Text](images/Versuch%204/LossF1MinimalData.png)

Fazit
-   nicht in der Lage die Verteilung über gesamten Raum zu lernen
-   Globale Struktur wird im begrenzten Ausmaß erlernt
-   Datenpunkteskala passt nur bedingt
-   representiert die 'Realität' nicht ausreichend
-   Loss Function -> Training und Test Verlauf in Ordnung jedoch nicht
    überagend gut

## Versuch 3.1.2

Bild Fuktion ![Alt-Text](images/Versuch%204/GroundtruthF4)

Bild Fuktion zugeschnitten

Bild Datenpunkte zufällig Training
![Alt-Text](images/Versuch%204/DataGenerationRandomTrainF4MinimalData)

Bild Datenpunkte zufällig Test
![Alt-Text](images/Versuch%204/DataGenerationRandomTestF4MinimalData)

Bild Datenpunkte VAE
![Alt-Text](images/Versuch%204/VAEDataF4MinimalData)

Bild Loss Function ![Alt-Text](images/Versuch%204/LossF4MinimalData.png)

Fazit
-   nicht in der Lage die Verteilung über gesamten Raum zu lernen
-   Globale Struktur wird nicht erkannt
-   Datenpunkteskala passt nur bedingt
-   representiert die 'Realität' nicht
-   Loss Function -> Training und Test Verlauf in Ordnung jedoch nicht
    überagend gut

-> Versuch abgebrochen

# Versuch 3.3
## Versuch 3.3.1 

Bild Fuktion ![Alt-Text](images/Versuch%204/GroundtruthF1)

Bild Fuktion zugeschnitten

Bild Datenpunkte zufällig Training
![Alt-Text](images/Versuch%204/DataGenerationRandomTrainF1MaximalData)

Bild Datenpunkte zufällig Test
![Alt-Text](images/Versuch%204/DataGenerationRandomTestF1MaximalData)

Bild Datenpunkte VAE
![Alt-Text](images/Versuch%204/VAEDataF1MaximalData)

Bild Loss Function ![Alt-Text](images/Versuch%204/LossF1MaximalData.png)

Fazit
-   bessere Performance + Ergebnisse als bei 50 Datenpunkten
-   Loss Function -> Training und Test Verlauf gut und benötigt viel
    weniger Epochen
-   lernt Verteilung an sich sehr gut, Problem: scheint
    Dimensionsreduktion anzuwenden, dadurch verzerrte / gestauchte
    Darstellung
-   nicht praktikabel für geplanten Einsatz

## Versuch 3.3.1 

Bild Fuktion ![Alt-Text](images/Versuch%204/GroundtruthF4)

Bild Fuktion zugeschnitten

Bild Datenpunkte zufällig Training
![Alt-Text](images/Versuch%204/DataGenerationRandomTrainF4MaximalData)

Bild Datenpunkte zufällig Test
![Alt-Text](images/Versuch%204/DataGenerationRandomTestF4MaximalData)

Bild Datenpunkte VAE
![Alt-Text](images/Versuch%204/VAEDataF4MaximalData)

Bild Loss Function ![Alt-Text](images/Versuch%204/LossF4MaximalData.png)

Fazit
-   bessere Performance + Ergebnisse als bei 50 Datenpunkten
-   Loss Function -\> Training und Test Verlauf gut und benötigt viel
    weniger Epochen
-   lernt Verteilung an sich sehr gut, Problem: scheint
    Dimensionsreduktion anzuwenden, dadurch verzerrte / gestauchte
    Darstellung
-   nicht praktikabel für geplanten Einsatz
-   (Annahme) lokale Struktur wird erkannt und representiert die
    Funktion

-> Versuch abgebrochen

|-> in Realität ist Groun Truth nicht bekannt, somit ist die Rekonstruktion nicht möglich 
|-> nicht für den angedachten Verwendungszweck nicht geeignet 
|-> Ausblick: Modell nochmals anschauen (Layer, Parameter, etc.), besseres Lernen der Funktion, GAN
oder CVAE ggf. ausprobieren

# andere Experimente
## Eperiment 1 und 2
...

# Schlussbetrachtung
## Fazit
...

## Kritische Diskussion
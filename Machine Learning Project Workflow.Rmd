---
title: "Machine Learning Project"
author: "Hammerer, Ibele, Janez, Romer, Steinwender"
date: "2023-07-29"
output:
  ioslides_presentation: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## requirements

```{r}
require(smoof)
require(ggplot2)
require(keras)
require(SPOT)
require(pracma)
require(lhs)
require(COBBS)
require(nloptr)

```


## adjustements

```{r}

set.seed(1)
numBbobf <- 24
dim <- 2
dataGenerationMethod <- "random" #"grid", "lhs"
numDataPoints <- 1000
trainTestSplit <- 0.8


```


## loading the function

```{r}

loadFunction <- function(numBbobf=1, dim=2) {
  f <- makeBBOBFunction(dim,numBbobf,1)
  return(f)
}


plot2DFunction <- function(f,lower,upper,vectorized=FALSE){
  x <- seq(from=lower[1],to=upper[1],length.out=100)
  y <- seq(from=lower[2],to=upper[2],length.out=100)
  df <-  expand.grid(x = x, y = y)
  if(vectorized)
    z <- f(df)
  else
    z <- apply(df,1,f)
  df$z <- z
  p <- ggplot(df, aes(x, y, z=z))
  p <- p +
    geom_contour_filled()
  p
}

```



```{r}

bbobf <- loadFunction(numBbobf=numBbobf,dim=dim)
lower <- getLowerBoxConstraints(bbobf)
upper <- getUpperBoxConstraints(bbobf)


if(dim==2){
  plot2DFunction(bbobf,lower,upper)
} else if (dim==3){
  NULL
}


```

## generating the data

It is not clear how many data points are needed. There must be enough data points to generate a model that is able to fit the data good enough but not too many data points to slow down the algorithm. The data can be generated randomly or with the help of a model.

```{r}

generateRandom <- function(n=50,lower,upper,dim=2){
  x <- runif(n,lower[1],upper[1])
  for (i in 2:dim) {
    x <- cbind(x, runif(n,lower[i],upper[i]))
  }
  x
}


generateGrid <- function(n = 50, lower, upper, dim = 2){
  grid_elements <- round(pracma::nthroot(n, dim))
  seq <- NULL
  for (i in 1:dim) {
    seq <- cbind(seq, seq(lower[i], upper[i], length.out=grid_elements))
  }
  if(dim == 2){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2])
  } else if(dim == 3){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3])
  } else if(dim == 4){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3], x1 = seq[,4])
  }
  data
}



generateLHS <- function(n = 50, lower, upper, dim = 2){
  x <- randomLHS(n, dim)
  for(i in 1:dim){
    x[,i] <- lower + (upper-lower)*x[,i]
  }
  x
}


generateDataPoints <- function(n = 50, f, dim = 2, method = "random"){
  ftest <- f 
  lower <- getLowerBoxConstraints(ftest)
  upper <- getUpperBoxConstraints(ftest)
  if (method == "random") {
    x <- generateRandom(n = n, lower = lower, upper = upper, dim = dim)
  } else if (method == "grid") {
    x <- generateGrid(n = n, lower = lower, upper = upper, dim = dim)
  } else if (method == "lhs") {
    x <- generateLHS(n = n, lower = lower, upper = upper, dim = dim)
  } else {
    stop("wrong method - please choose 'random' or 'lhs'")
  }
  df <-  data.frame(x)
  df$z <- apply(df,1,ftest)
  return(df)
}


plot2DDataPoints <- function(df){
  dfPlot <- data.frame(x=df[, 1],y=df[, 2],z=df[, 3])
  ggplot(data=dfPlot,aes(x=x,y=y,colour=z)) +
    geom_point() +
    scale_colour_gradientn(colours= c("#440154", "#414487", "#2a788e", "#22a884", "#7ad151", "#fde725"))
}


```

```{r}


df <- generateDataPoints(n = numDataPoints, bbobf, dim = dim, method = dataGenerationMethod)
if(dim==2){
  plot2DDataPoints(df)
}


```


```{r}

custom_loss <- function(y_true, y_pred) {
  # Implement your custom loss calculation here
  # For example, you can use mean squared error with a weighting factor
  weight_factor <- 0.001
  mse <- mean((y_true - y_pred)^2)
  weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
  return(weighted_mse)
}


createRNNModel <- function(){
  model = keras_model_sequential() %>% 
   layer_dense(units=128, input_shape=2) %>% 
   layer_activation_leaky_relu() %>% 
   layer_dense(units=32) %>% 
   layer_activation_leaky_relu() %>% 
   layer_dense(units=128) %>% 
   layer_activation_leaky_relu() %>%
#   layer_dropout(rate=0.01) %>%
#   layer_dense(units=64, activation="relu", input_shape=2) %>% 
#   layer_dense(units=64, activation="relu", input_shape=2) %>% 
#   layer_dense(units=32, activation = "relu") %>% 
#   layer_dropout(rate=0.001) %>% 
#   layer_dense(units=64) %>% 
#   layer_activation_leaky_relu() %>% 
   layer_dense(units=1, activation="linear")
 
  model %>% compile(
     loss = "mse",
     optimizer =  "adam", 
     metrics = list("mean_absolute_error")
   )
   
  model %>% summary()
  
  model
}


```



```{r}
splitTrainTest <- function(df, dim = dim, tts = 0.8){
  l <- nrow(df)
  s <- l * tts
  X_train <- df[0:s, 1:dim]
  X_train <- data.matrix(X_train)
  y_train <- df[0:s, (dim + 1)]
  X_test <- df[(s+1):l, 1:dim]
  X_test <- data.matrix(X_test)
  y_test <- df[(s+1):l, (dim + 1)]
  list(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test)
}

data <- splitTrainTest(df, dim, trainTestSplit)
X_train <- data$X_train
y_train <- data$y_train
X_test <- data$X_test
y_test <- data$y_test


```



```{r}


model_rnn_random <- createRNNModel()

model_rnn_random %>% fit(X_train, y_train, epochs = 150,verbose = 0)

y_pred = model_rnn_random %>% predict(X_test)



plotModel <- function(model, lower, upper){
  x <- seq(from=lower[1],to=upper[1],length.out=100)
  y <- seq(from=lower[2],to=upper[2],length.out=100)
  
  df <-  expand.grid(x = x, y = y)
  
  x_pred <- cbind(df$x, df$y)
  
  z <- model %>% predict(x_pred)
  
  df$z <- z
  
  p <- ggplot(df, aes(x, y, z=z))
  p <- p + geom_contour_filled()
  
  p
  
}


plotModel(model_rnn_random, lower, upper)


```



```{r}

require(kernlab)
fitted_gpr_random <- kernlab::gausspr(X_train_random,y_train_random)

plotModel(fitted_gpr_random,lower,upper)
```





## optimization




```{r}

#BFGSinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
#  con<-list(funEvals=200,populationSize= (100 * length(lower)),trace=0)
#  con[names(control)] <- control
#  control<-con
#  funEvals <- control$funEvals
#  NP <- control$populationSize
	
#  ## recalculate funEvals to DE iteration on basis of population size
#  itermax <- floor((funEvals - NP) / NP)
#  if(itermax < 1) itermax= 1
	
#  #control$NP <- control$populationSize
#  control$maxit <- 2000
#  control$pgtol <- 1e-9999999999999999999
#  control$factr <- 1e-9999999999999999999
#  control$lmm <- 2000
	
#	## Delete unused settings
#  control$populationSize <- NULL
#  #control$funEvals <- NULL
#  #control$types <- NULL
	
#  ## wrapper for matrix inputs to fun
#  fn <- function(x,...)fun(matrix(x,1),...) 

#  ## start optim
#  res <- optimLBFGSB(fun=fn,lower=lower,upper=upper,control=control,...)
#  list(x=res$member$bestmemit,y=res$member$bestvalit,xbest=res$xbest,ybest=res$ybest,count= res$optim$nfeval*nrow(res$member$pop))
#}


```




```{r}

## specify some model configuration
mc  <- list(useLambda=F,thetaLower=1e-6,thetaUpper=1e12)

## and some configuration details for the simulation
cntrl <- list(modelControl=mc,
              nsim=1,
              seed=1,
              method="spectral",
              Ncos = 100,
              conditionalSimulation=TRUE
)

```

```{r}
## Log results from a single optimization run to generate training data.
exprDE <- expression(
  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)

```

```{r}
## Log results from a single optimization run to generate training data.
#exprLBFGSB <- expression(
#  res <- BFGSinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
#)

```

```{r}
## Log results from a single optimization run to generate training data.
exprDE2 <- expression(
  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=10))
)

```



```{r}
## Log results from a single optimization run to generate training data.
exprDE3 <- expression(
  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*40))
)

```


## Benchmark mit Optimierungsalgorithmen

Wir wollen also zeigen, dass die beobachteten Unterschiede durchaus relevant für Optimierungsbenchmarks sind.
Die folgenden Graphen zeigen jeweils die Abweichung zwischen der *Performance*
auf der Ground-Truth, in Vergleich zu der *Performance* von verschiedenen Modellen / Simulationen.

```{r}

require(COBBS)

resgtde <- loggedExperiment(exprDE, groundtruth, 1,logx = TRUE)
resgtde
resgtde <- resgtde[1:(dimension*50),]
resgtde

x <- as.matrix(resgtde[,c(4,5)]) # training data: features
y <- as.matrix(resgtde[,2,drop=F]) # training data: observations



```

```{r}


## generate some ground-truth via BBOB


groundtruth <- function(x){
  x=matrix(x,,2) 
  apply(x,1,bbobf)
}

groundtruth(x)


```






```{r}

## generate model and test functions from the data -> generating a model / function
cobbsResult <- generateCOBBS(x,y,cntrl)
cobbsResult$fit

```




```{r}

## also: 2-level model
#cobbsFit2 <- COBBS::gaussianProcessR2L(x, y, control = list(useLambda=F))
#cobbsResult2 <- COBBS::simulateFunction(cobbsFit2,seed = 1,nsim=1,Ncos = 100,conditionalSimulation = #T)

# second plot shows the first function of the simulation - not much smoothing - overall quite good
#plotf(cobbsResult2[[1]],lower,upper,vectorized=T)


## prepare an expression that will be run during the experiments
## here: DE
# DEinterface is used for minimization of a function that used the DEoptim algorithm
# → Minimization for Differential Evolution
# "interface" for the differential evolution → any other optimization algotirhm could be used

#exprDE <- expression(
#  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=1000*dimension))
#)

```

```{r}


#exprLBFGSB <- expression(
#  res <- LBFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=1000*dimension))
#)

```


```{r}
## run the experiments, with logging
## with each objective function produced by COBBS

# from the ground truth
resgtde <- loggedExperiment(exprDE, groundtruth, 1:10,10)
resgtde

```




```{r}

# from the estimation of cobbs result
resesde <- loggedExperiment(exprDE, cobbsResult$estimation, 1:10,10)
resesde

```



```{r, echo=FALSE, message=FALSE, results='hide'}

# from the rnn  result
resnnde <- loggedExperiment(exprDE, model_rnn_random$predict, 1:10,10)

resnnde

```







```{r}

## plot results
print(plotBenchmarkPerformance(list(resgtde,resesde,resnnde),
                    c("groundtruth","gaussestimation","nnreg")))


## plot error, comparing against groundtruth
print(plotBenchmarkValidation(resgtde,list(resesde,resnnde),
                    c("gaussestimation","nnreg")))
```



```{r}


resgt <- loggedExperiment(exprDE2, groundtruth, 1:10,10)
resgt


```



```{r}

# from the estimation of cobbs result
reses <- loggedExperiment(exprDE2, cobbsResult$estimation, 1:10,10)
reses

```




```{r}

# from the rnn  result
resnn <- loggedExperiment(exprDE2, model_rnn_random$predict, 1:10,10)

resnn

```







```{r}

## plot results
print(plotBenchmarkPerformance(list(reses,resnn,resgt),
                    c("gaussestimation","nnreg","groundtruth")))


## plot results
print(plotBenchmarkPerformance(list(resnn, reses),
                    c("nnreg","gaussestimation")))

## plot results
print(plotBenchmarkPerformance(list(resnn,resgt),
                    c("nnreg","groundtruth")))


## plot error, comparing against groundtruth
print(plotBenchmarkValidation(resgt,list(reses,resnn),
                    c("gaussestimation","nnreg")))

```





```{r}


```




```{r}




```



```{r}



```


```{r}


```



```{r}





```

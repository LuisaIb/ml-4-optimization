---
title: "Machine Learning Project"
author: "Hammerer, Ibele, Janez, Romer, Steinwender"
date: "2023-07-29"
output:
  ioslides_presentation: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## requirements

```{r}
require(smoof)
require(ggplot2)
require(keras)
require(SPOT)
require(pracma)
require(lhs)
require(cmaes)
```





## loading the functions

```{r}
set.seed(1)

loadFunction <- function(num, dim=2) {
  f <- makeBBOBFunction(dim,num,1)
  return(f)
}


plotFunction <- function(f,lower,upper,vectorized=FALSE){
  x <- seq(from=lower[1],to=upper[1],length.out=100)
  y <- seq(from=lower[2],to=upper[2],length.out=100)
  df <-  expand.grid(x = x, y = y)
  if(vectorized)
    z <- f(df)
  else
    z <- apply(df,1,f)
  #z <- log10(z-min(z)+1)
  df$z <- z
  p <- ggplot(df, aes(x, y, z=z))
  p <- p + geom_contour_filled()
  #p <- p + geom_raster(aes(fill=z))
  #p <- p + scale_fill_gradientn(values=c(min(z), median(z), max(z)), colours=c("red","white","blue"))
  p
}

```

```{r}

f1 <- loadFunction(num=1)
f2 <- loadFunction(num=3)
f3 <- loadFunction(num=23)
f4 <- loadFunction(num=24)



plotFunction(f1,getLowerBoxConstraints(f1),getUpperBoxConstraints(f1))
plotFunction(f2,getLowerBoxConstraints(f2),getUpperBoxConstraints(f2))
plotFunction(f3,getLowerBoxConstraints(f3),getUpperBoxConstraints(f3))
plotFunction(f4,getLowerBoxConstraints(f4),getUpperBoxConstraints(f4))


```

## generating the data

It is not clear how many data points are needed. There must be enough data points to generate a model that is able to fit the data good enough but not too many data points to slow down the algorithm. The data can be generated randomly or with the help of a model.

```{r}

generateRandom <- function(n = 50, lower, upper, dim = 2){
  x <- runif(n,lower[1],upper[1])
  for (i in 2:dim) {
    x <- cbind(x, runif(n,lower[i],upper[i]))
  }
  x
}


generateGrid <- function(n = 50, lower, upper, dim = 2){
  grid_elements <- round(pracma::nthroot(n, dim))
  seq <- NULL
  for (i in 1:dim) {
    seq <- cbind(seq, seq(lower[i], upper[i], length.out=grid_elements))
  }
  if(dim == 2){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2])
  } else if(dim == 3){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3])
  } else if(dim == 4){
    data <- expand.grid(x1 = seq[,1], x2 = seq[,2], x1 = seq[,3], x1 = seq[,4])
  }
  data
}



generateLHS <- function(n = 50, lower, upper, dim = 2){
  x <- randomLHS(n, dim)
  for(i in 1:dim){
    x[,i] <- lower + (upper-lower)*x[,i]
  }
  x
}


generateDataPoints <- function(n = 50, f, dim = 2, method = "random"){
  ftest <- f 
  lower <- getLowerBoxConstraints(ftest)
  upper <- getUpperBoxConstraints(ftest)
  if (method == "random") {
    x <- generateRandom(n = n, lower = lower, upper = upper, dim = dim)
  } else if (method == "grid") {
    x <- generateGrid(n = n, lower = lower, upper = upper, dim = dim)
  } else if (method == "lhs") {
    x <- generateLHS(n = n, lower = lower, upper = upper, dim = dim)
  } else {
    stop("wrong method - please choose 'random' or 'lhs'")
  }
  df <-  data.frame(x)
  df$z <- apply(df,1,ftest)
  return(df)
}


plot2DDataPoints <- function(df){
  dfPlot <- data.frame(x=df[, 1],y=df[, 2],z=df[, 3])
  ggplot(data=dfPlot,aes(x=x,y=y,colour=z)) +
    geom_point() +
    scale_colour_gradientn(colours=rainbow(4))
}


```

```{r}

df_random <- generateDataPoints(n = 200, f1, dim = 2, method = "random")

plot2DDataPoints(df_random)


df_grid <- generateDataPoints(n = 200, f1, dim = 2, method = "grid")

plot2DDataPoints(df_grid)


df_lhs <- generateDataPoints(n = 200, f1, dim = 2, method = "lhs")

plot2DDataPoints(df_lhs)


```


```{r}

custom_loss <- function(y_true, y_pred) {
  # Implement your custom loss calculation here
  # For example, you can use mean squared error with a weighting factor
  weight_factor <- 0.001
  mse <- mean((y_true - y_pred)^2)
  weighted_mse <- mean((y_true - y_pred)^2 * (y_true^weight_factor))
  return(weighted_mse)
}


# Create a Keras loss function from your custom function
loss_function <- custom_loss

createRNNModel <- function(){
  model = keras_model_sequential() %>% 
   layer_dense(units=128, input_shape=2) %>% 
   layer_activation_leaky_relu() %>% 
#   layer_dense(units=32, input_shape=2) %>% 
#   layer_activation_leaky_relu() %>% 
#   layer_dense(units=128, input_shape=2) %>% 
#   layer_activation_leaky_relu() %>%
   layer_dropout(rate=0.01) %>%
#   layer_dense(units=64, activation="relu", input_shape=2) %>% 
#   layer_dense(units=64, activation="relu", input_shape=2) %>% 
#   layer_dense(units=32, activation = "relu") %>% 
#   layer_dropout(rate=0.001) %>% 
   layer_dense(units=1, activation="linear")
 
  model %>% compile(
     loss = "mse",
     optimizer =  "adam", 
     metrics = list("mean_absolute_error")
   )
   
  model %>% summary()
  
  model
}


```



```{r}
splitTrainTest <- function(df, dim = 2, tts = 0.8){
  l <- nrow(df)
  s <- l * tts
  X_train <- df[0:s, 1:dim]
  X_train <- data.matrix(X_train)
  y_train <- df[0:s, (dim + 1)]
  X_test <- df[(s+1):l, 1:dim]
  X_test <- data.matrix(X_test)
  y_test <- df[(s+1):l, (dim + 1)]
  list(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test)
}


data_random <- splitTrainTest(df_random)
X_train_random <- data_random$X_train
y_train_random <- data_random$y_train
X_test_random <- data_random$X_test
y_test_random <- data_random$y_test


data_grid <- splitTrainTest(df_grid)
X_train_grid <- data_grid$X_train
y_train_grid <- data_grid$y_train
X_test_grid <- data_grid$X_test
y_test_grid <- data_grid$y_test


data_lhs <- splitTrainTest(df_lhs)
X_train_lhs <- data_lhs$X_train
y_train_lhs <- data_lhs$y_train
X_test_lhs <- data_lhs$X_test
y_test_lhs <- data_lhs$y_test



```



```{r}

model_rnn_random <- createRNNModel()

model_rnn_random %>% fit(X_train_random, y_train_random, epochs = 150,verbose = 0)


y_pred = model_rnn_random %>% predict(X_test)



plotModel <- function(f, model){
  lower <- getLowerBoxConstraints(f)
  upper <- getUpperBoxConstraints(f)
  
  x <- seq(from=lower[1],to=upper[1],length.out=100)
  y <- seq(from=lower[2],to=upper[2],length.out=100)
  
  df <-  expand.grid(x = x, y = y)
  
  x_pred <- cbind(df$x, df$y)
  
  z <- model %>% predict(x_pred)
  
  df$z <- z
  
  p <- ggplot(df, aes(x, y, z=z))
  p <- p + geom_contour_filled()

  
  p
  
}


plotModel(f1, model_rnn_random)


```



```{r}

require(kernlab)
fitted_gpr_random <- kernlab::gausspr(X_train_random,y_train_random)

fpred_random <- function(X_test_random){
  predict(fitted_gpr_random, X_test_random)
}

plotFunction(fpred_random,lower,upper,vectorized=T)
```





## optimization


```{r}
library(bbotk)
library(mlr3)
library(mlr3learners)
library(mlr3mbo)

MBOEinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
  con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0)
  con[names(control)] <- control
  control<-con
  funEvals <- control$funEvals
  NP <- control$populationSize
	
  ## recalculate funEvals to DE iteration on basis of population size
  itermax <- floor((funEvals - NP) / NP)
  if(itermax < 1) itermax= 1
	
  control$NP <- control$populationSize
  control$itermax <- itermax
	
	## Delete unused settings
  control$populationSize <- NULL
  control$funEvals <- NULL
  control$types <- NULL
	
  ## wrapper for matrix inputs to fun
  fn <- function(x,...)fun(matrix(x,1),...) 

  ## start optim
  
  
  surrogate = SurrogateLearner$new(lrn("regr.km"))
  acq_optimizer = AcqOptimizer$new(opt("random_search"), terminator = trm("evals"))
  
  
  
  res <- OptimizerMbo$new(bayesopt_ego,
    surrogate = surrogate,
    acq_function = fn,
    acq_optimizer = acq_optimizer)
  list(x=res$member$bestmemit,y=res$member$bestvalit,xbest=matrix(res$optim$bestmem,1),ybest=matrix(res$optim$bestval,1),count= res$optim$nfeval*nrow(res$member$pop))
}

```


```{r}

```


```{r}

LVFGSBinterface <-function(x=NULL,fun,lower,upper,control=list(),...){
  con<-list(funEvals=200,populationSize= (10 * length(lower)),trace=0)
  con[names(control)] <- control
  control<-con
  funEvals <- control$funEvals
  #NP <- control$populationSize
	
  ## recalculate funEvals to DE iteration on basis of population size
  #itermax <- floor((funEvals - NP) / NP)
  #if(itermax < 1) itermax= 1
	
  #control$NP <- control$populationSize
  #control$itermax <- itermax
	
	## Delete unused settings
  control$populationSize <- NULL
  control$funEvals <- NULL
  control$types <- NULL
	
  ## wrapper for matrix inputs to fun
  fn <- function(x,...)fun(matrix(x,1),...) 

  ## start optim
  res <- optim(c(-4.5,-4.5), fn=fn, method = "L-BFGS-B", lower=lower,upper=upper,control=control,...)
  list(x=res$member$bestmemit,y=res$member$bestvalit,count= res$optim$nfeval*nrow(res$member$pop))
}

```




```{r}

## specify some model configuration
mc  <- list(useLambda=F,thetaLower=1e-6,thetaUpper=1e12)

## and some configuration details for the simulation
cntrl <- list(modelControl=mc,
              nsim=1,
              seed=1,
              method="spectral",
              Ncos = 100,
              conditionalSimulation=TRUE
)


# generate COBBS generates an estimation and a simulation of the function
#fpred <- COBBS::generateCOBBS(as.matrix(df[,-3]), matrix(df[,3],,1), control = cntrl)

#as.matrix(df[,-3])
#matrix(df[,3])


#fpred$estimation

# ploting the estimation
#plotf(fpred$estimation,lower,upper,vectorized=T)
```



## Benchmark mit Optimierungsalgorithmen

Wir wollen also zeigen, dass die beobachteten Unterschiede durchaus relevant für Optimierungsbenchmarks sind.
Die folgenden Graphen zeigen jeweils die Abweichung zwischen der *Performance*
auf der Ground-Truth, in Vergleich zu der *Performance* von verschiedenen Modellen / Simulationen.

```{r}
require(COBBS)

## generate some ground-truth via BBOB
require(smoof)
seed <- 1234
fnbbob <- f1

#x

groundtruth <- function(x){
  x=matrix(x,,2) 
  apply(x,1,fnbbob)
}

x <- seq(from=lower[1],to=upper[1],length.out=100)
groundtruth(x)

lower = getLowerBoxConstraints(fnbbob)
upper = getUpperBoxConstraints(fnbbob)
dimension <- length(lower)
set.seed(seed)

```

```{r}
## Log results from a single optimization run to generate training data.
expr <- expression(
  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=dimension*100,populationSize=dimension*20))
)

```


```{r}
expr

```



```{r}

require(COBBS)
resgt <- loggedExperiment(expr, groundtruth, 1,logx = TRUE)
resgt
resgt <- resgt[1:(dimension*50),]
resgt
x <- as.matrix(resgt[,c(4,5)]) # training data: features

print(x)

y <- as.matrix(resgt[,2,drop=F]) # training data: observations

print(y)

```



```{r}

## generate model and test functions from the data -> generating a model / function
cobbsResult <- generateCOBBS(x,y,cntrl)
cobbsResult$fit

```






```{r}

## also: 2-level model
#cobbsFit2 <- COBBS::gaussianProcessR2L(x, y, control = list(useLambda=F))
#cobbsResult2 <- COBBS::simulateFunction(cobbsFit2,seed = 1,nsim=1,Ncos = 100,conditionalSimulation = #T)

# second plot shows the first function of the simulation - not much smoothing - overall quite good
#plotf(cobbsResult2[[1]],lower,upper,vectorized=T)


## prepare an expression that will be run during the experiments
## here: DE
# DEinterface is used for minimization of a function that used the DEoptim algorithm
# → Minimization for Differential Evolution
# "interface" for the differential evolution → any other optimization algotirhm could be used
require(nloptr)
expr <- expression(
  res <- DEinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=1000*dimension))
)
## run the experiments, with logging
## with each objective function produced by COBBS

# from the ground truth
resgt <- loggedExperiment(expr, groundtruth, 1:10,10)
resgt

```



```{r}

cobbsResult$estimation

```


```{r}

# from the estimation of cobbs result
reses <- loggedExperiment(expr, cobbsResult$estimation, 1:10,10)
reses

```



```{r}

# from the rnn  result
resnn <- loggedExperiment(expr, model_rnn_random$predict, 1:10,10)

resnn

```







```{r}

## plot results
print(plotBenchmarkPerformance(list(resgt,reses,resnn),
                    c("groundtruth","gaussestimation","nnreg")))


## plot error, comparing against groundtruth
print(plotBenchmarkValidation(resgt,list(reses,resnn),
                    c("gaussestimation","nnreg")))
```



```{r}


exprLBFGSB <- expression(
  res <- LVFGSBinterface(fun = fnlog,lower=lower,upper=upper,control=list(funEvals=1000*dimension))
)
## run the experiments, with logging
## with each objective function produced by COBBS

# from the ground truth
resgt <- loggedExperiment(exprLBFGSB, groundtruth, 1:10,10)
resgt


```






```{r}

## generate model and test functions from the data -> generating a model / function
cobbsResult <- generateCOBBS(x,y,cntrl)
cobbsResult$fit

```



```{r}

# from the estimation of cobbs result
reses <- loggedExperiment(exprLBFGSB, cobbsResult$estimation, 1:10,10)
reses

```




```{r}

# from the rnn  result
resnn <- loggedExperiment(exprLBFGSB, model_rnn_random$predict, 1:10,10)

resnn

```







```{r}

## plot results
print(plotBenchmarkPerformance(list(resgt,reses,resnn),
                    c("groundtruth","gaussestimation","nnreg")))


## plot error, comparing against groundtruth
print(plotBenchmarkValidation(resgt,list(reses,resnn),
                    c("gaussestimation","nnreg")))
```





```{r}


```




```{r}




```



```{r}



```


```{r}


```



```{r}





```
